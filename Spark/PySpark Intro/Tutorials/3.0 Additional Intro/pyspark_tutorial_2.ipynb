{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "suited-target",
   "metadata": {},
   "source": [
    "# Tutorial 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "described-camera",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "statistical-desire",
   "metadata": {},
   "source": [
    "# Understanding closures \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-courtesy",
   "metadata": {},
   "source": [
    "<b>One of the harder things about Spark is understanding the scope and life cycle of variables and methods when executing code across a cluster.</b> RDD operations that modify variables outside of their scope can be a frequent source of confusion. In the example below we’ll look at code that uses foreach() to increment a counter, but similar issues can occur for other operations as well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "addressed-editing",
   "metadata": {},
   "source": [
    "\n",
    "Example\n",
    "Consider the naive RDD element sum below, which may behave differently depending on whether execution is happening within the same JVM. A common example of this is when running Spark in local mode (--master = local[n]) versus deploying a Spark application to a cluster (e.g. via spark-submit to YARN):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "hazardous-shareware",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "distData = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "minor-flash",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter value:  0\n"
     ]
    }
   ],
   "source": [
    "counter = 0\n",
    "rdd = sc.parallelize(data)\n",
    "\n",
    "# Wrong: Don't do this!!\n",
    "def increment_counter(x):\n",
    "    global counter\n",
    "    counter += x\n",
    "rdd.foreach(increment_counter)\n",
    "\n",
    "print(\"Counter value: \", counter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "interim-employment",
   "metadata": {},
   "source": [
    "Local vs. cluster modes\n",
    "The behavior of the above code is undefined, and may not work as intended. To execute jobs, Spark breaks up the processing of RDD operations into tasks, each of which is executed by an executor. Prior to execution, Spark computes the task’s closure. The closure is those variables and methods which must be visible for the executor to perform its computations on the RDD (in this case foreach()). This closure is serialized and sent to each executor.\n",
    "\n",
    "The variables within the closure sent to each executor are now copies and thus, when counter is referenced within the foreach function, it’s no longer the counter on the driver node. There is still a counter in the memory of the driver node but this is no longer visible to the executors! The executors only see the copy from the serialized closure. Thus, the final value of counter will still be zero since all operations on counter were referencing the value within the serialized closure.\n",
    "\n",
    "In local mode, in some circumstances the foreach function will actually execute within the same JVM as the driver and will reference the same original counter, and may actually update it.\n",
    "\n",
    "To ensure well-defined behavior in these sorts of scenarios one should use an Accumulator. Accumulators in Spark are used specifically to provide a mechanism for safely updating a variable when execution is split up across worker nodes in a cluster. The Accumulators section of this guide discusses these in more detail.\n",
    "\n",
    "In general, closures - constructs like loops or locally defined methods, should not be used to mutate some global state. Spark does not define or guarantee the behavior of mutations to objects referenced from outside of closures. Some code that does this may work in local mode, but that’s just by accident and such code will not behave as expected in distributed mode. Use an Accumulator instead if some global aggregation is needed.\n",
    "\n",
    "Printing elements of an RDD\n",
    "Another common idiom is attempting to print out the elements of an RDD using rdd.foreach(println) or rdd.map(println). On a single machine, this will generate the expected output and print all the RDD’s elements. However, in cluster mode, the output to stdout being called by the executors is now writing to the executor’s stdout instead, not the one on the driver, so stdout on the driver won’t show these! To print all elements on the driver, one can use the collect() method to first bring the RDD to the driver node thus: rdd.collect().foreach(println). This can cause the driver to run out of memory, though, because collect() fetches the entire RDD to a single machine; if you only need to print a few elements of the RDD, a safer approach is to use the take(): rdd.take(100).foreach(println)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-oklahoma",
   "metadata": {},
   "source": [
    "# Broadcast Variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "flush-declaration",
   "metadata": {},
   "source": [
    "Broadcast variables allow the programmer to keep a read-only variable cached on each machine rather than shipping a copy of it with tasks. They can be used, for example, to give every node a copy of a large input dataset in an efficient manner. Spark also attempts to distribute broadcast variables using efficient broadcast algorithms to reduce communication cost.\n",
    "\n",
    "Spark actions are executed through a set of stages, separated by distributed “shuffle” operations. Spark automatically broadcasts the common data needed by tasks within each stage. The data broadcasted this way is cached in serialized form and deserialized before running each task. This means that explicitly creating broadcast variables is only useful when tasks across multiple stages need the same data or when caching the data in deserialized form is important.\n",
    "\n",
    "Broadcast variables are created from a variable v by calling SparkContext.broadcast(v). The broadcast variable is a wrapper around v, and its value can be accessed by calling the value method. The code below shows this:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "decreased-scholarship",
   "metadata": {},
   "outputs": [],
   "source": [
    "broadcastVar = sc.broadcast([1, 2, 3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-patent",
   "metadata": {},
   "source": [
    "NOTE: Broadcast variables are used to save the copy of data across all nodes. This variable is cached on all the machines and not sent on machines with tasks. The following code block has the details of a Broadcast class for PySpark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "functional-document",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "broadcastVar.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "streaming-float",
   "metadata": {},
   "source": [
    "After the broadcast variable is created, it should be used instead of the value v in any functions run on the cluster so that v is not shipped to the nodes more than once. In addition, the object v should not be modified after it is broadcast in order to ensure that all nodes get the same value of the broadcast variable (e.g. if the variable is shipped to a new node later).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "genetic-fishing",
   "metadata": {},
   "source": [
    "# Accumulators\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spoken-ensemble",
   "metadata": {},
   "source": [
    "Accumulators are variables that are only “added” to through an associative and commutative operation and can therefore be efficiently supported in parallel. They can be used to implement counters (as in MapReduce) or sums. Spark natively supports accumulators of numeric types, and programmers can add support for new types.\n",
    "\n",
    "As a user, you can create named or unnamed accumulators. As seen in the image below, a named accumulator (in this instance counter) will display in the web UI for the stage that modifies that accumulator. Spark displays the value for each accumulator modified by a task in the “Tasks” table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "multiple-session",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "potential-dealer",
   "metadata": {},
   "outputs": [],
   "source": [
    "accum = sc.accumulator(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "transparent-olive",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Accumulator<id=0, value=0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "discrete-replica",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.parallelize([1, 2, 3, 4]).foreach(lambda x: accum.add(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "rolled-father",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "progressive-duncan",
   "metadata": {},
   "source": [
    "While this code used the built-in support for accumulators of type Int, programmers can also create their own types by subclassing AccumulatorParam. The AccumulatorParam interface has two methods: zero for providing a “zero value” for your data type, and addInPlace for adding two values together. For example, supposing we had a Vector class representing mathematical vectors, we could write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "nutritional-marker",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.accumulators import AccumulatorParam\n",
    "class VectorAccumulatorParam(AccumulatorParam):\n",
    "    def zero(self, value):\n",
    "        return [0.0] * len(value)\n",
    "    def addInPlace(self, val1, val2):\n",
    "        for i in range(len(val1)):\n",
    "             val1[i] += val2[i]\n",
    "        return val1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "broke-compensation",
   "metadata": {},
   "source": [
    "Then, create an Accumulator of this type:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "crazy-congo",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 2.0, 3.0]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#va = sc.accumulator(Vector(...), VectorAccumulatorParam())  ## USAGE\n",
    "va = sc.accumulator([1.0, 2.0, 3.0], VectorAccumulatorParam())\n",
    "va.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "premium-embassy",
   "metadata": {},
   "outputs": [],
   "source": [
    "def g(x):\n",
    "    global va\n",
    "    va += [x] * 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "comfortable-madagascar",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7.0, 8.0, 9.0]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.parallelize([1,2,3])\n",
    "rdd.foreach(g)\n",
    "va.value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "theoretical-mounting",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "regulation-march",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "broken-frank",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[9] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accum = sc.accumulator(0)\n",
    "def g(x):\n",
    "    accum.add(x)\n",
    "    return f(x)\n",
    "distData.map(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-season",
   "metadata": {},
   "source": [
    "# map(f, preservesPartitioning = False)\n",
    "A new RDD is returned by applying a function to each element in the RDD. In the following example, we form a key value pair and map every string with a value of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "educated-solution",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Key value pair:\n",
      " [('scala', 1), ('java', 1), ('hadoop', 1), ('spark', 1), ('spark', 1), ('akka', 1), ('spark vs hadoop', 1), ('pyspark', 1), ('pyspark and spark', 1)]\n"
     ]
    }
   ],
   "source": [
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"spark\",\n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "words_map = words.map(lambda x: (x, 1))\n",
    "mapping = words_map.collect()\n",
    "print(\"Key value pair:\\n %s\" % (mapping))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "outdoor-tuning",
   "metadata": {},
   "source": [
    "# reduce(f)\n",
    "After performing the specified commutative and associative binary operation, the element in the RDD is returned. In the following example, we are importing add package from the operator and applying it on ‘num’ to carry out a simple addition operation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "realistic-tobago",
   "metadata": {},
   "outputs": [],
   "source": [
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "homeless-sight",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adding all the elements: 15\n"
     ]
    }
   ],
   "source": [
    "nums = sc.parallelize([1, 2, 3, 4, 5])\n",
    "adding = nums.reduce(add)\n",
    "print(\"Adding all the elements: %i\" % (adding))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-prize",
   "metadata": {},
   "source": [
    "# join(other, numPartitions = None)\n",
    "It returns RDD with a pair of elements with the matching keys and all the values for that particular key. In the following example, there are two pair of elements in two different RDDs. After joining these two RDDs, we get an RDD with elements having matching keys and their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "strategic-average",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = sc.parallelize([(\"spark\", 1), (\"hadoop\", 4)])\n",
    "y = sc.parallelize([(\"spark\", 2), (\"hadoop\", 5)])\n",
    "joined = x.join(y)\n",
    "final = joined.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bibliographic-volunteer",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join RDD -> [('spark', (1, 2)), ('hadoop', (4, 5))]\n"
     ]
    }
   ],
   "source": [
    "print(\"Join RDD -> %s\" % (final))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "olive-python",
   "metadata": {},
   "source": [
    "# cache()\n",
    "Persist this RDD with the default storage level (MEMORY_ONLY). You can also check if the RDD is cached or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "sophisticated-single",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words got cached: True\n"
     ]
    }
   ],
   "source": [
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ") \n",
    "words.cache() \n",
    "caching = words.persist().is_cached \n",
    "print(\"Words got cached: %s\" % (caching))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protective-transparency",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-package",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
