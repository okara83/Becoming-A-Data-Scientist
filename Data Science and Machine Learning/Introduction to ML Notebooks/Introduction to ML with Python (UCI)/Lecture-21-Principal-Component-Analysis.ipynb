{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 21: Principal Component Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy.linalg as LA\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA and data preprocessing\n",
    "\n",
    "Principal Components Analysis (PCA) is a dimensionality reduction algorithm that can be used to significantly speed up our feature(s) learning algorithm. Mathematically speaking, PCA uses a method called Singular Value Decomposition (SVD), in which the singular value resembles the eigenvalue.\n",
    "\n",
    "Suppose we are training a model to classify our MNIST handwritten digits (28x28 grayscale images). A given training sample in `X_train` is of shape `(784,)`, which has 784 features (dimensions). However, many of these features are somewhat redundant, because the values of adjacent pixels in an image are highly correlated. Concretely, for a training vector $\\mathbf{x} = (x_1,\\dots, x_{784}) \\in \\mathbb{R}^{784}$ which is 784 dimensional vectors, with each feature $x_j$ corresponding to the intensity of $j$-th pixel (in the flattened image). Because of the correlation between adjacent pixels, PCA will allow us to approximate the input with a much lower dimensional one, while incurring very little error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## (Reading) What does PCA exactly do?\n",
    "\n",
    "PCA will find the most significant \"direction\" in a dataset, then the second most significant direction, then the third most significant direction,.... so on and so forth.\n",
    "\n",
    "Let $A\\in \\mathbb{R}^{n\\times d}$ matrix, usually we assume $n>d$\n",
    "\n",
    ">Singular Value Decomposition (SVD): any real matrix can be decomposed into the following form:\n",
    ">$$A = U S V^{\\top}$$\n",
    "\n",
    "Where $S\\in \\mathbb{R}^{n\\times d}$ is a diagonal matrix whose diagonal entries are non-negative and in decreasing order. $U\\in \\mathbb{R}^{n\\times n}$ and $V\\in \\mathbb{R}^{d\\times d}$ are orthogonal matrices (i.e. columns of $V$ are orthonormal, same for $U$, $U^{\\top}U = UU^{\\top} = I$). \n",
    "\n",
    "The columns of $V= [\\mathbf{v}_1 \\mathbf{v}_2 \\cdots \\mathbf{v}_d]$ are the significant directions we were looking for, which are known as the right singular vectors. Moreover, the columns of $V$ form a $d$-dimensional orthogonal basis.\n",
    "\n",
    "The columns of $U = [\\mathbf{u}_1 \\mathbf{u}_2 \\cdots \\mathbf{u}_n]$ are known as the left singular vectors. \n",
    "\n",
    "Sometimes we just say \"eigenvectors\" instead of \"singular vectors\".\n",
    "\n",
    "If some singular values are 0 or very small, we can essentially \"discard\" those singular values and the corresponding eigenvectors, and still get a reasonably good approximation of our data, hence reducing the dimensions.\n",
    "\n",
    "Finally, $U$ (or more precisely $U S$) stores how you write each coordinate vector in terms of the significant directions in $V$. \n",
    "\n",
    "----\n",
    "\n",
    "### Geometric meaning\n",
    "\n",
    "Consider what happens to the unit sphere in our vector space as it isbeing transformed by the matrix $X$. First, we apply some transformation $V^{\\top}$, which is essentially a rotation, since $V^{\\top}$ is a matrix with orthonormal rows. \n",
    "\n",
    "A matrix with orthonormal rows just changes the coordinate axes via some rotation or reflection but does no scaling.\n",
    "\n",
    "Next, we apply a scaling defined by $S$, which just scales the dimensions since it is a diagonal matrix. \n",
    "\n",
    "Finally, we rotate again with $U$. In other words, any transformation can be expressed as a rotation followed by a scaling followed by another rotation. \n",
    "\n",
    "----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rank-$k$ approximation\n",
    "\n",
    "The vectors $V= [\\mathbf{v}_1 \\mathbf{v}_2 \\cdots \\mathbf{v}_d]$ are such that they describe the most important axis of the data in the following sense. The first eigenvector $\\mathbf{v}_1$ describes which direction has the most variance. Then since $\\mathbf{v}_2, \\cdots, \\mathbf{v}_d$ are each orthogonal to $\\mathbf{v}_1$, this implies that $\\mathbf{v}_2$\n",
    "is the direction (after $\\mathbf{v}_1$ has been factored out) that has the most variance.\n",
    "\n",
    "The rank $k$ approximation $A_k \\in \\mathbb{R}^{n\\times d}$ of $A$ is as follows: \n",
    "$$A_k = U_k S_k V_k^{\\top},$$\n",
    "where $S_k$ is still an $\\mathbb{R}^{n\\times d}$ matrix, but only with the first $k$ entries being nonzero. In this way, only first $k$ columns of $V$ ($k$ rows of $V^{\\top}$) contribute to $A_k$.\n",
    "\n",
    "----\n",
    "\n",
    "## Example of Gauss\n",
    "\n",
    "Let us recall in the beginning of this quarter, and load `Gauss.jpg`, then flatten its color dimension to make a grayscale image.\n",
    "\n",
    "Reference: [https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.linalg.svd.html](https://docs.scipy.org/doc/numpy-1.15.1/reference/generated/numpy.linalg.svd.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = plt.imread(\"gauss.jpg\")\n",
    "G_bw = np.mean(G, axis=2)\n",
    "plt.imshow(G_bw, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, VT = LA.svd(G_bw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S_mat = np.zeros_like(G_bw)\n",
    "S_mat[:600] = np.diag(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.allclose(G_bw, U.dot(S_mat.dot(VT)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remark: Or you can use `@`, where the `@` (at) operator is intended to be used for matrix multiplication. This is New in version 3.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# suppose we only use first k singular values\n",
    "k = 30\n",
    "S_k = np.zeros_like(G_bw)\n",
    "S_k[:k, :k] = np.diag(S[:k])\n",
    "G_k = (U @ S_k) @ VT\n",
    "\n",
    "plt.imshow(G_k, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why can we discard the smaller singular values?\n",
    "\n",
    "Let us use the following artificially generated data as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = 500\n",
    "mean = np.array([0,0])\n",
    "covariance = np.array([[12,  9], [ 9, 10]])\n",
    "X = np.random.multivariate_normal(mean, covariance, num_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, VT = LA.svd(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VT[0,:] # first eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VT[1,:] # second eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v1 = VT[0,:] * S[0]/20\n",
    "v2 = VT[1,:] * S[1]/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.axis([-8,8,-8,8])\n",
    "plt.scatter(X[:,0], X[:,1], alpha=0.2)\n",
    "plt.arrow(0, 0, v1[0], v1[1], width=0.1, head_width=0.34, head_length=0.8)\n",
    "plt.arrow(0, 0, v2[0], v2[1], width=0.1, head_width=0.34, head_length=0.8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What does the \"significance\" of a direction mean in data science?\n",
    "\n",
    "\n",
    "Consider using SVD on the dataset matrix $X\\in \\mathbb{R}^{N\\times d}$, where $X$'s $i$-th row corresponds to the $i$-th data sample. There are $N$ data samples and each data point has $n$ dimensions (features). If the data set is normalized row-wise, i.e., each row (data sample) has mean zero. The significant directions (columns of $V$) are the eigenvectors of the covariance matrix ${\\frac{1}{N-1}X^T X}$. The singular values (entries of $S$) are the square roots of the eigenvalues of this covariance matrix.\n",
    "\n",
    "In other words, $S$'s entries are the amount of deviations of the data in that direction. The covariance matrix tells you how correlated each direction is with other directions in the space where the dataset lives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In-class Exercise: MNIST\n",
    "PCA can be used to speed up a machine learning algorithm (logistic regression) on the MNIST dataset.\n",
    "\n",
    "Download `mnist_binary_train.npz` and `mnist_binary_test.npz` on Canvas files tab, and load them using the following cell. Then try the following:\n",
    "\n",
    "* Use `scikit-learn`'s `LogisticRgression` class on the original dataset.\n",
    "* Import `PCA` from `scikit-learn`'s `decomposition` submodule, apply it on the dataset, re-run the `LogisticRegression` on the reduced dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = np.load('mnist_binary_train.npz')\n",
    "data_test = np.load('mnist_binary_test.npz')\n",
    "X_train, y_train = data_train['X'], data_train['y']\n",
    "X_test, y_test = data_test['X'], data_test['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use scikit-learn's built-in logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time # measure the time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_reg = LogisticRegression(solver='lbfgs', max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_time  = time.process_time()\n",
    "mnist_reg.fit(X_train,y_train)\n",
    "print(\"Data fitting takes\", time.process_time() - starting_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_reg.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use PCA to reduce the dimension\n",
    "\n",
    "From the [reference of the `PCA`](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html):\n",
    "> If `0 < n_components < 1` and `svd_solver == 'full'`, select the number of components such that the amount of variance that needs to be explained is greater than the percentage specified by n_components.\n",
    "\n",
    "The following cell means that the `PCA` class in `scikit-learn` will choose the minimum number of principal components such that 95% of the variance is retained for our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_pca = PCA(n_components=0.95)\n",
    "mnist_pca.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reduced = mnist_pca.transform(X_train)\n",
    "X_test_reduced = mnist_pca.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_reg_pca = LogisticRegression(solver='lbfgs', max_iter=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "starting_time  = time.process_time()\n",
    "mnist_reg_pca.fit(X_train_reduced,y_train)\n",
    "print(\"Data fitting takes\", time.process_time() - starting_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_reg_pca.score(X_test_reduced,y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
