{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 21 Classification IV: k-nearest neighbors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# $k$-nearest neighbors ($k$NN) classifier\n",
    "\n",
    "Setting: \n",
    "\n",
    "A simple classifier adopting the Bayesian thinking (a posteriori, without assuming a priori any model) is *kNN*. kNN can be thought as an approximation to the naive Bayes classifier. \n",
    "\n",
    "We will try to estimate the conditional probability of $y=j$ given training/testing sample $\\mathbf{x}$ for each $j=1,\\dots, K$, and then classify a given training/testing to the class with highest *estimated* probability.\n",
    "\n",
    "Given a $k$ (different from the index for the class) and a test sample $\\mathbf{x}$, the kNN classifier first identifies the neighbors $k$ points in the training data that are closest to $\\mathbf{x}$, whose indices are represented by $\\mathcal{N}$. It then estimates the conditional probability for class $j$ by computing the fraction of points in $\\mathcal{N}$ whose label(s) actually equal $j$:\n",
    "\n",
    "$$\n",
    "P\\big(y= j| \\mathbf{x} \\big)\\approx  \\frac{1}{k} \\sum_{i\\in \\mathcal{N}} 1\\{ y^{(i)} = j\\}.\n",
    "$$\n",
    "\n",
    "Finally, kNN applies Bayesian rule and classifies the test sample $\\mathbf{x}$ to the class with the largest estimated probability (the class with the most *votes* from those $k$-nearest neighbors). Despite the fact that it is a very simple approach, kNN can often produce classifiers that are surprisingly close to the optimal naive Bayes classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of $k$NN\n",
    "\n",
    "Even the formulation above is pretty mathematical, the algorithm reads very simple as follows:\n",
    "\n",
    "> Step 1: Assign a distance metric to the training samples $\\{\\mathbf{x}^{(i)} \\}_{i=1}^N$ ($L^2$, $L^1$, $L^{\\infty}$, etc), and choose the number of neighbors $k$.<br>\n",
    "> Step 2: For the new data sample, take the $k$ nearest neighbors of this sample in $\\{\\mathbf{x}^{(i)} \\}_{i=1}^N$, according to the distance metric.<br>\n",
    "> Step 3: Among these $k$ neighbors, count the number of data samples in each class.<br>\n",
    "> Step 4: Assign this new data sample to the class where the most neighbors are counted.\n",
    "\n",
    "Remark: To avoid tie situations, we note that $k$ is usually odd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A famous example: classify flowers using the lengths of petals/sepals\n",
    "\n",
    "The data set we are using is the [Iris Flower Dataset (IFD)](https://www.kaggle.com/uciml/iris).Here we will use `scikit-learn`'s dataset module to import it.\n",
    "\n",
    "IFD was first introduced in 1936 by the famous statistician Ronald Fisher and consists of 50 observations from each of three species of Iris (Iris setosa, Iris virginica and Iris versicolor). Four features were measured from each sample: the length and the width of the sepals and petals. Our goal is to train a kNN algorithm to be able to distinguish the species from one another given the measurements of the 4 features.\n",
    "\n",
    "In the in-class presentation, we will use `scikit-learn`'s built-in kNN classifier. A full implementation using `numpy` is the in the end of this notebook for you to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# if we load the raw Iris.csv, we have to pre-process the data\n",
    "iris_data = pd.read_csv('Iris.csv')\n",
    "iris_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we drop the Id column using the drop function in pandas Dataframe class\n",
    "# then visualize the dataset using seaborn.\n",
    "sns.pairplot(iris_data.drop(labels = ['Id'], axis=1), hue='Species')  # dropping the Id column\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set()\n",
    "sns.pairplot(iris_data.drop(labels = ['Id'], axis=1), diag_kind='hist', hue='Species')  # dropping the Id column\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using scikit-learn\n",
    "\n",
    "Below we load the data from `scikit-learn`, since the label is already pre-processed as numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "iris_data_proc = load_iris()\n",
    "iris_data_proc.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = iris_data_proc.data, iris_data_proc.target\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## kNN from scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading library\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate learning model (k = 5)\n",
    "iris_knn = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting the model\n",
    "iris_knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the response\n",
    "y_pred = iris_knn.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate accuracy\n",
    "print(\"The testing accuracy is: \", np.mean(y_test == y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In-class exercise:\n",
    "\n",
    "* Read the [reference of `KNeighborsClassifier`](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html).\n",
    "* Try different number of `n_neighbors` (odd or even numbers), what have you observed.\n",
    "* There is an option `weights ='distance'` in the `KNeighborsClassifier` class, try it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement the $k$NN algorithm\n",
    "\n",
    "Following the main working pipeline we have seen in regression, we need to implement the following three components:\n",
    "\n",
    "* Model: choose the distance metric we want to use, choose $k$\n",
    "* Training: there is actually no training since there is no model, just store the data.\n",
    "* Testing/Cross-validate: compute the distance between a testing sample and all training samples. Sorting and get the $k$ nearest neighbors, and perform a majority to determine the class.\n",
    "\n",
    "### Distance choosing\n",
    "The most commonly used is the Euclidean distance ($L^2$-distance):\n",
    "$$\n",
    "d(\\mathbf{x}, \\mathbf{x}') = \\sqrt{(x_1 - x'_1 )^2 + (x_2 - x'_2 )^2 + \\dots + (x_n - x'_n )^2},\n",
    "$$\n",
    "for other distances, please refer to [the DistanceMetric class in scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.DistanceMetric.html). The next popular one is Manhattan distance ($L^1$-distance), which performs much better than $L^2$ in high dimension (the data point has many features).\n",
    "\n",
    "First we notice that sorting the $L^2$ distance $d(\\mathbf{x}, \\mathbf{x}')$ is like sorting the distance squared, so we do not have to take square root just to save some time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a simple kNN implementation\n",
    "M = len(y_test)\n",
    "N = len(y_train)\n",
    "k = 5 # no. of neighbors to a testing sample\n",
    "y_pred = np.zeros(M, dtype = int) # the prediction of the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(M):\n",
    "    dist_j = np.zeros(N)\n",
    "    # initialization: dist_j stores the distances of j-th testing sample to all training samples\n",
    "    for i in range(N):\n",
    "        dist_j[i] = np.sum((X_test[j,:] - X_train[i,:])**2)\n",
    "        # dist_j[i] is the distance of j-th testing sample to i-th training sample\n",
    "    idx_knn = np.argsort(dist_j)[:k] # return the indices of k nearest neighbors\n",
    "    # X_train[idx_knn[0], :] will be the training sample that is closest to j-th testing sample\n",
    "    label_neighbor = y_train[idx_knn]\n",
    "    # y_train[idx_knn] is the labels of these k nearest neighbor\n",
    "    label_neighbor, count_label = np.unique(label_neighbor, return_counts=True)\n",
    "    # count_label: how many time a certain labeled samples appear as in the k nearest neighbors\n",
    "    y_pred[j] = label_neighbor[np.argmax(count_label)]\n",
    "    # np.argmax(count_label) returns the index of the label appearing \n",
    "\n",
    "# evaluate accuracy\n",
    "print(\"The testing accuracy is: \", np.mean(y_test == y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usages of sort, argsort, and unique in numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examples of argsort vs sort\n",
    "arr = np.array([1, 0, 5, 19, 12, 3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sort(arr) # returns the sorted array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argsort(arr) # returns the indices of the sorted entry\n",
    "# the first entry 1 means element 1 in arr is 0, the smallest entry\n",
    "# the second entry 0 means element 0 in arr is 1, the second smallest entry\n",
    "# the j-th entry in the argsort means element j in arr is the j-th smallest entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# an example of np.unique\n",
    "label_neighbor, count_label = np.unique([2, 1, 1, 1, 1, 1, 0, 0], return_counts=True)\n",
    "print(label_neighbor) # the unique labels in the array\n",
    "print(count_label) # how many times each labels appearing in the array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.argmax(count_label) # the most frequent label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading: Vectorization of kNN\n",
    "A common way to implement $k$NN is to run a `for` loop for all testing samples. In $i$-th iteration of the loop, the distances between the $i$-th testing sample and all training samples are computed and sorted, so that we can choose the $k$-smallest distance neighbors. This is highly un-vectorized, and not recommended for large dataset.\n",
    "\n",
    "Instead we consider the problem in bulk, suppose we have $n$ features in our dataset: we need to find the $L^2$-distance squared between the a set of test vectors (representing the testing samples), stored in array `X_test` of shape `(M,n)`, and a set of training vectors (representing the training samples), held in a matrix `X_train` of shape `(N,n)`. Our goal is to create a distance matrix `D` of shape `(M,N)` that stores the $L^2$-distance squared from every test vector to every training vector, for example `D[j,i]` shall return the distance squared between the $j$-th testing sample and the $i$-th training sample, and `D[j,:]` the $j$-th row of `D` stores the distance squared between the $j$-th testing sample and all training samples.\n",
    "\n",
    "To do this in a vectorized way, a simple for $L^2$ distance is using $|x-x'|^2 = x^2 -2xx' + x'^2$. A vectorized implementation with no `for` loop iterating around all testing samples is in the following cell, for moderate dataset like MNIST, the performance is about several hundred times faster than using `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dist[j,i] is the np.sum((X_test[j,:] - X_train[i,:])**2)\n",
    "Dist = -2 * np.dot(X_test, X_train.T) + np.sum(X_train**2, axis=1) + np.sum(X_test**2, axis=1).reshape(-1,1)\n",
    "# try testing each term's shape in a new cell and ponder about this implementation\n",
    "# notice np.sum(X_test**2, axis=1).reshape(-1,1) is the same with np.sum(X_test**2, axis=1)[:, np.newaxis]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now the implementation is almost the same with above\n",
    "k = 5 # k nearest neighbors\n",
    "K = 3 # K classes\n",
    "idx_knn = Dist.argsort(axis = 1)[:,:k] # sort by columns for each row, then return the first k columns\n",
    "knn_label = y_train[idx_knn]\n",
    "# the vectorized version takes some tricks\n",
    "count_label = np.zeros((len(y_test), K), dtype = int)\n",
    "for j in range(K): # a small for loop iterating on classes\n",
    "    count_label[:,j] = np.sum((knn_label==j), axis=1)\n",
    "y_pred = np.argmax(count_label, axis=1)\n",
    "\n",
    "# evaluate accuracy\n",
    "print(\"The testing accuracy is: \", np.mean(y_test == y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some cells below are for you to figure out what happened above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(X_test**2, axis=1).reshape(-1,1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.dot(X_test, X_train.T).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(X_train**2, axis=1).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[idx_knn][:10,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_label[:10,]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
