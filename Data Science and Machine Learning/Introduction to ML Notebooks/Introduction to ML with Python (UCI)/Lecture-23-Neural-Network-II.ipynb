{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 25: Multi-layer Neural Network II\n",
    "\n",
    "### How to train a neural net\n",
    "Today we will learn how to apply the gradient descent algorithm on the regression problem for the model function $h(\\mathbf{x}; W, b)$.\n",
    "\n",
    "\n",
    "#### References:\n",
    "\n",
    "\n",
    "* [Simple MNIST numpy from scratch](https://www.kaggle.com/scaomath/simple-mnist-numpy-from-scratch)\n",
    "* [Stanford Deep Learning Tutorial in Matlab](http://ufldl.stanford.edu/tutorial/supervised/MultiLayerNeuralNetworks/)\n",
    "* [3Blue1Brown's video series on Deep Learning](https://www.youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi)\n",
    "* [A visual proof that neural nets can compute any function](http://neuralnetworksanddeeplearning.com/chap4.html)\n",
    "* [Backpropagation and chain rule](http://colah.github.io/posts/2015-08-Backprop/)\n",
    "* [Chapter 6: Multiplayer Feedforward Neural Network in *Deep Learning Book* by Goodfellow et al](http://www.deeplearningbook.org/contents/mlp.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "<img src=\"https://faculty.sites.uci.edu/shuhaocao/files/2019/04/neural_net_3l.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "The model function $h(\\mathbf{x}; W, b) = \\mathbf{a}^{(3)} = f(\\mathbf{z}^{(3)})$ in the network above, and\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbf{z}^{(l+1)} &= W^{(l)} \\mathbf{a}^{(l)} + \\mathbf{b}^{(l)}   \\\\\n",
    "\\mathbf{a}^{(l+1)} &= f(\\mathbf{z}^{(l+1)})\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review\n",
    "\n",
    "<img src=\"https://faculty.sites.uci.edu/shuhaocao/files/2019/04/neural_net_3l.png\" alt=\"drawing\" width=\"600\"/>\n",
    "\n",
    "The loss function \n",
    "$$\n",
    "J(W,b; X, \\mathbf{y})\n",
    "= \\frac{1}{N} \\sum_{m=1}^N \\left( \\frac{1}{2} \\left\\| h(\\mathbf{x}^{(m)}; W,b) - y^{(i)} \\right\\|^2 \\right) + \\frac{\\epsilon}{2} \\sum_{l=1}^{n_l-1} \\; \\sum_{i=1}^{s_l} \\; \\sum_{j=1}^{s_{l+1}} \\left( w^{(l)}_{ji} \\right)^2,\n",
    "$$\n",
    "where $n_l$ denote the number of layers, and $s_l$ denote the number of nodes in layer $l$ (not counting the bias). We can write the loss function for a single sample $(\\mathbf{x}^{(i)}, y^{(i)})$ (sample loss) as \n",
    "$$\n",
    "J(W,b; \\mathbf{x}^{(m)}, y^{(m)}) = \\frac{1}{2} \\big\\| h(\\mathbf{x}^{(m)}; W,b) -  y^{(m)} \\big\\|^2,\n",
    "$$\n",
    "then the loss function can be written as:\n",
    "$$\n",
    "J(W,b; X, \\mathbf{y}) = (\\text{Mean of sample losses} ) + (\\text{Regularization for weights} )\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training of the neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Initialization\n",
    "To train our neural network, we will initialize each parameter $w^{(l)}_{ij}$ and each $b^{(l)}_i$ to a small random value near zero, and then apply an optimization algorithm such as mini-batch stochastic gradient descent. Since $J(W,b)$ is a non-convex function, gradient descent-based methods without any correction (e.g., AdaBoost) is susceptible to local optima; however, in practice gradient descent usually works fairly well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient descent\n",
    "\n",
    "On iteration of gradient descent reads: $W = \\big( w_{ij}^{(l)}\\big)_{l=1}^{s_l - 1}$ (total $s_l$ layers, and $s_l-1$ set of weights between neighboring layers), and $b = \\big( b_{i}^{(l)}\\big)_{l=1}^{s_l - 1}$ (bias on all layers except the output layer),\n",
    "> $$\\begin{aligned}\n",
    "w_{ij}^{(l)} &\\gets w_{ij}^{(l)} - \\eta \\frac{\\partial}{\\partial w_{ij}^{(l)}} J(W,b) \\\\\n",
    "b_{i}^{(l)} &\\gets b_{i}^{(l)} - \\eta \\frac{\\partial}{\\partial b_{i}^{(l)}} J(W,b)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation\n",
    "\n",
    "Recall that the sample loss is $J(W,b; \\mathbf{x}, y) $, we can write the partial derivatives above as:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial w_{ij}^{(l)}} J(W,b) \n",
    "&=\n",
    "\\left\\{ \\frac{1}{N} \\sum_{m=1}^N \\frac{\\partial}{\\partial w_{ij}^{(l)}} J(W,b; \\mathbf{x}^{(m)}, y^{(m)}) \\right\\} + \\epsilon w_{ij}^{(l)} \\\\\n",
    "\\frac{\\partial}{\\partial b_{i}^{(l)}} J(W,b) &=\n",
    "\\frac{1}{N}\\sum_{m=1}^N \\frac{\\partial}{\\partial b_{i}^{(l)}} J(W,b; \\mathbf{x}^{(m)}, y^{(m)})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "The intuition behind the backpropagation algorithm is as follows. Given a training example $(\\mathbf{x}, y)$, we will first run a \"forward pass\" to compute all the activations $a_i^{(l)}$ throughout the network for each unit $i$ in every layer $l$, including the output value of the model $h(\\mathbf{x};W,b)$. Then, for each unit $i$ in layer $l$, we would like to compute an \"error term\" $\\delta^{(l)}_i$ that measures how much that node was \"responsible\" for any errors in our output. For an output node, we can directly measure the difference between the network's activation and the true target value, and use that to define $\\delta^{(n_l)}_i$ (where layer $n_l$ is the output layer). For hidden units, we will compute $\\delta^{(l)}_i$ based on a weighted average of the error terms of the units that uses $a_i^{(l)}$ as an input. In detail, here is the backpropagation algorithm:\n",
    "\n",
    "> Step 1: Perform a forward pass, computing the activations for layers $L_2$, $L_3$, and so on up to the output layer $L_{n_l}$.\n",
    ">\n",
    "> Step 2: For each output unit $i$ in layer $n_l$ (the output layer), set\n",
    "$$\n",
    "\\delta^{(n_l)}_i\n",
    ":= \\frac{\\partial}{\\partial z^{(n_l)}_i} \\;J(W,b; \\mathbf{x}, y) = \\frac{\\partial}{\\partial z^{(n_l)}_i} \n",
    "\\frac{1}{2} \\left\\|y - h(\\mathbf{x};W,b)\\right\\|^2 = - (y - a^{(n_l)}_i) \\cdot f'(z^{(n_l)}_i)\n",
    "$$\n",
    ">\n",
    "> Step 3: For $l=n_l−1,n_l−2,n_l−3,\\dots,2$ For each node $i$ in layer $l$, set\n",
    "$$\\delta^{(l)}_i : = \\frac{\\partial}{\\partial z^{(l)}_i} \\;J(W,b; \\mathbf{x}, y) =\n",
    "\\frac{\\partial}{\\partial a^{(l)}_i} \\;J(W,b; \\mathbf{x}, y)  \n",
    "\\frac{\\partial a^{(l)}_i}{\\partial z^{(l)}_i}=  \\left( \\sum_{j=1}^{s_{l+1}} w^{(l)}_{ji} \\delta^{(l+1)}_j \\right) f'(z^{(l)}_i)\n",
    "$$\n",
    ">\n",
    "> Step 4: the desired partial derivatives are computed as:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial}{\\partial w_{ij}^{(l)}} J(W,b; \\mathbf{x}, y) &= a^{(l)}_j \\delta_i^{(l+1)} \\\\\n",
    "\\frac{\\partial}{\\partial b_{i}^{(l)}} J(W,b; \\mathbf{x}, y) &= \\delta_i^{(l+1)}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detailed derivation using chain rule\n",
    "\n",
    "In this cell, we are going to derive the detailed formula for the derivatives of each weight in the simple 3-layer neural net. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorized version\n",
    "We will use $\\ast$ to denote the element-wise product operator (which is exactly `*` operand for numpy arrays), so that if $\\mathbf{a}= \\mathbf{b}\\ast \\mathbf{c}$, then $a_i=b_i c_i$. \n",
    "\n",
    "Similarly to last lecture we can extend the definition of $f(\\cdot)$ to apply element-wise to vectors, so that $f'([z_1, z_2, z_3]) = [f'(z_1), f'(z_2), f'(z_3)]$.\n",
    "\n",
    "Using the notations above, the algorithm can then be rewritten:\n",
    "\n",
    "> Step 1: Perform a forward pass, computing the activations for layers $L_2$, $L_3$, and so on up to the output layer $L_{n_l}$.\n",
    ">\n",
    "> Step 2: For each output unit $i$ in layer $n_l$ (the output layer), set\n",
    "$$\n",
    "\\boldsymbol{\\delta}^{(n_l)} = - (\\mathbf{y} - \\mathbf{a}^{(n_l)}) \\ast f'(\\mathbf{z}^{(n_l)})\n",
    "$$\n",
    ">\n",
    "> Step 3: For $l=n_l−1,n_l−2,n_l−3,\\dots,2$ For each node $i$ in layer $l$, set\n",
    "$$\n",
    "\\boldsymbol{\\delta}^{(l)} = \\left((W^{(l)})^{\\top} \\boldsymbol{\\delta}^{(l+1)}\\right) \\ast f'(\\mathbf{z}^{(l)}) \n",
    "$$\n",
    ">\n",
    "> Step 4: the desired partial derivatives are computed as:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_{W^{(l)}} J(W,b;\\mathbf{x},y) &= \\boldsymbol{\\delta}^{(l+1)} (\\mathbf{a}^{(l)})^T, \\\\\n",
    "\\nabla_{b^{(l)}} J(W,b;\\mathbf{x},y) &= \\boldsymbol{\\delta}^{(l+1)}.\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation remarks\n",
    "\n",
    "In steps 2 and 3 above in either vectorized version or for loop version, we need to compute $f'(z^{(l)}_i)$ for each value of $i$. \n",
    "* When $f(z)$ is ReLU activation function, its derivative is just 0 when its input is less than zero and 1 when its input is greater than or equal to zero.\n",
    "* When $f(z)$ is the sigmoid activation function, we would already have $a^{(l)}_i$ stored away from the forward pass through the network. Thus, using the expression that we worked out earlier for $f′(z)$, we can compute this as $f'(z^{(l)}_i) = a^{(l)}_i (1- a^{(l)}_i)$.\n",
    "\n",
    "Finally, we are ready to describe the gradient descent algorithm for all samples. In the algorithm below, \n",
    "* $\\Delta W^{(l)}$ is a matrix of the same dimension as $W^{(l)}$. Recall that $W^{(l)}$ is the matrix of weights mapping the layer $l$ activation $\\mathbf{a}^{(l)}$ to match the size of layer $(l+1)$), \n",
    "* $\\Delta \\mathbf{b}^{(l)}$ is a vector (of the same dimension as $\\mathbf{b}^{(l)}$). \n",
    "\n",
    "One iteration of gradient descent is then as follows:\n",
    "> Step 1: $\\Delta W^{(l)} := 0$ and $\\Delta \\mathbf{b}^{(l)} := 0$.\n",
    ">\n",
    "> Step 2: For $m=1$ to N, use backpropagation to compute $\\nabla_{W^{(l)}} J(W,b;\\mathbf{x}^{(k)},y^{(k)})$ and $\\nabla_{\\mathbf{b}^{(l)}} J(W,b;\\mathbf{x}^{(k)},y^{(k)})$ for $k$-th sample, then\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Delta W^{(l)} & \\gets \\Delta W^{(l)} + \\nabla_{W^{(l)}} J(W,b;\\mathbf{x}^{(k)},y^{(k)})\n",
    "\\\\\n",
    "\\Delta \\mathbf{b}^{(l)} & \\gets \\Delta \\mathbf{b}^{(l)} + \\nabla_{b^{(l)}} J(W,b;\\mathbf{x}^{(k)},y^{(k)})\n",
    "\\end{aligned}\n",
    "$$\n",
    ">\n",
    "> Step 3: update the weights and biases\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W^{(l)} &= W^{(l)} - \\eta \\left\\{ \\left(\\frac{1}{N} \\Delta W^{(l)} \\right) + \\epsilon W^{(l)}\\right\\} \\\\\n",
    "\\mathbf{b}^{(l)} &= \\mathbf{b}^{(l)} - \\eta \\left\\{\\frac{1}{N} \\Delta \\mathbf{b}^{(l)}\\right\\}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy implementation from scratch\n",
    "\n",
    "Below is the implementation for the least-square loss. It unfortunately does not work for the classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu(x):\n",
    "    return x*(x>0)\n",
    "\n",
    "def relu_prime(x):\n",
    "    return 1.0*(x>0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W[0].shape = (n_features, 256), W[0] maps the input layer to the hidden layer\n",
    "# W[1].shape = (256, n_target), W[1] maps the output from the hidden layer to the output layer\n",
    "# b is the bias in each layer, it is also a list so that\n",
    "# b[0].shape = (256,) and b[0] is the bias in the input layer\n",
    "# b[1].shape = (n_target,) and b[1] is the bias in the hidden layer\n",
    "def h(X,W,b):\n",
    "    # layer 1 = input layer\n",
    "    a1 = X\n",
    "    # layer 1 (input layer) -> layer 2 (hidden layer)\n",
    "    z2 = np.matmul(X, W[0]) + b[0]\n",
    "    # layer 2 activation\n",
    "    a2 = relu(z2)\n",
    "    # layer 2 (hidden layer) -> layer 3 (output layer)\n",
    "    z3 = np.matmul(a2, W[1]) + b[1]\n",
    "    # output layer does not need activation for regression\n",
    "    output = z3\n",
    "    return output\n",
    "\n",
    "eps = 1e-4\n",
    "def loss(W,b,X,y):\n",
    "    '''\n",
    "    means square error loss\n",
    "    '''\n",
    "    residual = h(X,W,b) - y\n",
    "    regularization = eps*(np.sum(W[1]**2) + np.sum(W[0]**2))\n",
    "    return np.mean(residual**2) + regularization\n",
    "\n",
    "def backprop(W,b,X,y):\n",
    "    '''\n",
    "    Step 1: forward pass\n",
    "    Step 2: backprop\n",
    "    '''\n",
    "    N = X.shape[0]\n",
    "    # layer 1 = input layer\n",
    "    a1 = X\n",
    "    # layer 1 (input layer) -> layer 2 (hidden layer)\n",
    "    z2 = np.matmul(X, W[0]) + b[0]\n",
    "    # layer 2 activation\n",
    "    a2 = relu(z2)\n",
    "    # layer 2 (hidden layer) -> layer 3 (output layer)\n",
    "    z3 = np.matmul(a2, W[1]) + b[1]\n",
    "    # output layer activation\n",
    "    output = z3\n",
    "    \n",
    "    # layer 3's derivative\n",
    "    delta3 = -(y - output)\n",
    "    # layer 2's derivative\n",
    "    delta2 = np.matmul(delta3, W[1].T)*relu_prime(z2)\n",
    "    # no derivative for layer 1\n",
    "    \n",
    "    dW = [np.matmul(a1.T,delta2)/N + eps*W[0], np.matmul(a2.T,delta3)/N + eps*W[1]]\n",
    "    db = [np.mean(delta2, axis=0), np.mean(delta3, axis=0)]\n",
    "    # dW[0] is W[0]'s derivative, and dW[1] is W[1]'s derivative; similar for db\n",
    "    return dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# artificial generating data\n",
    "# N is the sample size (or current mini-batch size); \n",
    "# D_in is input dimension;\n",
    "# N_H is hidden dimension; \n",
    "# D_out is output dimension.\n",
    "N, D_in, N_H, D_out = 10000, 1000, 256, 10\n",
    "X = np.random.randn(N, D_in)\n",
    "y = np.random.randn(N, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "eta = 1e-4# step size (learning rate)\n",
    "num_steps = 20\n",
    "\n",
    "# initialization\n",
    "W = [np.random.randn(D_in, N_H), np.random.randn(N_H,D_out)]\n",
    "b = [np.random.randn(N_H), np.random.randn(D_out)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "137163.9663086516"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss(W,b,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "loss_at_eachstep = np.zeros(num_steps) # record the change of the loss function\n",
    "\n",
    "for i in range(num_steps):\n",
    "    loss_at_eachstep[i] = loss(W,b,X,y)\n",
    "    dW, db = backprop(W,b,X,y)\n",
    "    W[0] -= eta * dW[0]\n",
    "    W[1] -= eta * dW[1]\n",
    "    b[0] -= eta * db[0]\n",
    "    b[1] -= eta * db[1]\n",
    "    if i % 5 == 0:\n",
    "        print(\"loss after\", i+1, \"iterations is: \", loss_at_eachstep[i])\n",
    "        print(\"R squared\", i+1, \"iterations is: \", 1 - loss_at_eachstep[i]/(np.sum((y- np.mean(y))**2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise by yourself (very hard)\n",
    "\n",
    "The softmax regression alone did not fit the MNIST training set well, due to lack of parameters. In the neural network version, the output unit of is identical to the softmax regression function we have seen in leture of softmax regression. As a result, the cost function is nearly identical to the softmax regression cost function. Note that instead of making predictions from the input data $\\mathbf{x}$ the softmax function takes as input the final hidden layer of the neural network $h(\\mathbf{x}; W, b)$. The loss function is thus,\n",
    "$$\n",
    "J(\\theta) = - \\frac{1}{N}\\left\\{ \\sum_{i=1}^{N} \\sum_{k=1}^{K}  1\\left\\{y^{(i)} = k\\right\\} \n",
    "\\log \\frac{\\exp\\big(\\theta^{(k)\\top} h(\\mathbf{x}^{(i)}; W, b) \\big)}\n",
    "{\\sum_{j=1}^K \\exp\\big(\\theta^{(j)\\top} h(\\mathbf{x}^{(i)}; W, b) \\big)}\\right\\}.\n",
    "$$\n",
    "Note since the final hidden layer of the neural network has size `(256,)`, thus $\\theta$ should be of dimension `(256,10)`. The $\\delta^{(n_l)}$ are different now:\n",
    "$$\n",
    "\\delta^{(n_l)} = - \\frac{1}{N}\\sum_{i=1}^{N} \\Big\\{  1\\{ y^{(i)} = k\\}  - P(y^{(i)} = k | x^{(i)}; \\theta)  \\Big\\} \n",
    "$$\n",
    "However, if we still use ReLU activation for the previous layers, the $\\delta^{(l)}$s for $l=n_l−1,n_l−2,n_l−3,\\dots,2$ remain unchanged.\n",
    "\n",
    "* Train and test various network architectures. You should be able to achieve 100% training set accuracy with a single hidden layer of 256 hidden units. Because the network has many parameters, there is a danger of overfitting. Experiment with layer size, number of hidden layers (less than or equal to 3), and regularization penalty to understand what types of architectures perform best. Can you find a network with multiple hidden layers which outperforms your best single hidden layer architecture?\n",
    "* (Optional) Read the code and algorithms in [Simple MNIST numpy from scratch](https://www.kaggle.com/scaomath/simple-mnist-numpy-from-scratch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "X = np.load('mnist_train.npz')['X']/255\n",
    "y = np.load('mnist_train.npz')['y'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take just the first 10k sample and change the label to its one-hot encoding\n",
    "X = X[:10000]\n",
    "y = y[:10000]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": true,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
