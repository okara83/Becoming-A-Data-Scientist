{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 01 NLP INCLASS HANDS ON 20201219"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\turbulence\\anaconda3\\lib\\site-packages (3.5)\n",
      "Requirement already satisfied: click in c:\\users\\turbulence\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex in c:\\users\\turbulence\\anaconda3\\lib\\site-packages (from nltk) (2020.11.13)\n",
      "Requirement already satisfied: joblib in c:\\users\\turbulence\\anaconda3\\lib\\site-packages (from nltk) (0.17.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\turbulence\\anaconda3\\lib\\site-packages (from nltk) (4.54.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Downloading gensim-3.8.3-cp37-cp37m-win_amd64.whl (24.2 MB)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\turbulence\\anaconda3\\lib\\site-packages (from gensim) (1.19.3)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\users\\turbulence\\anaconda3\\lib\\site-packages (from gensim) (1.5.2)\n",
      "Requirement already satisfied: six>=1.5.0 in c:\\users\\turbulence\\anaconda3\\lib\\site-packages (from gensim) (1.15.0)\n",
      "Collecting Cython==0.29.14\n",
      "  Downloading Cython-0.29.14-cp37-cp37m-win_amd64.whl (1.7 MB)\n",
      "Requirement already satisfied: numpy>=1.11.3 in c:\\users\\turbulence\\anaconda3\\lib\\site-packages (from gensim) (1.19.3)\n",
      "Collecting smart-open>=1.8.1\n",
      "  Downloading smart_open-4.0.1.tar.gz (117 kB)\n",
      "Building wheels for collected packages: smart-open\n",
      "  Building wheel for smart-open (setup.py): started\n",
      "  Building wheel for smart-open (setup.py): finished with status 'done'\n",
      "  Created wheel for smart-open: filename=smart_open-4.0.1-py3-none-any.whl size=108248 sha256=c519bd91f04bbad4a625452e14951ff6bbb84f81592777dd1fb2ed0dd0fd85b6\n",
      "  Stored in directory: c:\\users\\turbulence\\appdata\\local\\pip\\cache\\wheels\\34\\3d\\14\\f19c01a19c9201cdb6a76b049904d5226912569be919ad1eae\n",
      "Successfully built smart-open\n",
      "Installing collected packages: smart-open, Cython, gensim\n",
      "  Attempting uninstall: Cython\n",
      "    Found existing installation: Cython 0.29.21\n",
      "    Uninstalling Cython-0.29.21:\n",
      "      Successfully uninstalled Cython-0.29.21\n",
      "Successfully installed Cython-0.29.14 gensim-3.8.3 smart-open-4.0.1\n",
      "Collecting spaCy\n",
      "  Downloading spacy-2.3.5-cp37-cp37m-win_amd64.whl (9.5 MB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\turbulence\\anaconda3\\lib\\site-packages (from spaCy) (4.54.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\turbulence\\anaconda3\\lib\\site-packages (from spaCy) (51.0.0.post20201207)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in c:\\users\\turbulence\\anaconda3\\lib\\site-packages (from spaCy) (2.25.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\turbulence\\anaconda3\\lib\\site-packages (from spaCy) (1.19.3)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.4-cp37-cp37m-win_amd64.whl (6.5 MB)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\turbulence\\anaconda3\\lib\\site-packages (from spaCy) (1.19.3)\n",
      "Collecting catalogue<1.1.0,>=0.0.7\n",
      "  Downloading catalogue-1.0.0-py2.py3-none-any.whl (7.7 kB)\n",
      "Requirement already satisfied: importlib-metadata>=0.20 in c:\\users\\turbulence\\anaconda3\\lib\\site-packages (from catalogue<1.1.0,>=0.0.7->spaCy) (2.0.0)\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.5-cp37-cp37m-win_amd64.whl (35 kB)\n",
      "Requirement already satisfied: zipp>=0.5 in c:\\users\\turbulence\\anaconda3\\lib\\site-packages (from importlib-metadata>=0.20->catalogue<1.1.0,>=0.0.7->spaCy) (3.4.0)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.5-cp37-cp37m-win_amd64.whl (20 kB)\n",
      "Collecting plac<1.2.0,>=0.9.6\n",
      "  Downloading plac-1.1.3-py2.py3-none-any.whl (20 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.5-cp37-cp37m-win_amd64.whl (108 kB)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in c:\\users\\turbulence\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spaCy) (3.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\turbulence\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spaCy) (1.25.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\turbulence\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spaCy) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\users\\turbulence\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.13.0->spaCy) (2.10)\n",
      "Collecting srsly<1.1.0,>=1.0.2\n",
      "  Downloading srsly-1.0.5-cp37-cp37m-win_amd64.whl (176 kB)\n",
      "Collecting thinc<7.5.0,>=7.4.1\n",
      "  Downloading thinc-7.4.5-cp37-cp37m-win_amd64.whl (888 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in c:\\users\\turbulence\\anaconda3\\lib\\site-packages (from spaCy) (4.54.1)\n",
      "Requirement already satisfied: numpy>=1.15.0 in c:\\users\\turbulence\\anaconda3\\lib\\site-packages (from spaCy) (1.19.3)\n",
      "Collecting wasabi<1.1.0,>=0.4.0\n",
      "  Downloading wasabi-0.8.0-py3-none-any.whl (23 kB)\n",
      "Installing collected packages: murmurhash, cymem, wasabi, srsly, preshed, plac, catalogue, blis, thinc, spaCy\n",
      "Successfully installed blis-0.7.4 catalogue-1.0.0 cymem-2.0.5 murmurhash-1.0.5 plac-1.1.3 preshed-3.0.5 spaCy-2.3.5 srsly-1.0.5 thinc-7.4.5 wasabi-0.8.0\n"
     ]
    }
   ],
   "source": [
    "!pip install gensim\n",
    "!pip install spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\TURBULENCE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\TURBULENCE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_text = \"Oh man, this is pretty cool. We will do more such things\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_tokens = word_tokenize(sample_text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oh', 'man', ',', 'this', 'is', 'pretty', 'cool', '.', 'we', 'will', 'do', 'more', 'such', 'things']\n"
     ]
    }
   ],
   "source": [
    "print(text_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REMOVING PUNCTUATION AND NUMBER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_without_punc = [w for w in text_tokens if w.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oh', 'man', 'this', 'is', 'pretty', 'cool', 'we', 'will', 'do', 'more', 'such', 'things']\n"
     ]
    }
   ],
   "source": [
    "print(tokens_without_punc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REMOVING STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_without_sw = [t for t in tokens_without_punc if t not in stop_words ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oh', 'man', 'pretty', 'cool', 'things']\n"
     ]
    }
   ],
   "source": [
    "print(tokens_without_sw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## STEMMING OR LEMMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\TURBULENCE\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\wordnet.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "st = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "lems = [lem.lemmatize(t) for t in tokens_without_sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oh', 'man', 'pretty', 'cool', 'thing']\n"
     ]
    }
   ],
   "source": [
    "print(lems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stems = [st.stem(t) for t in tokens_without_sw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['oh', 'man', 'pretti', 'cool', 'thing']\n"
     ]
    }
   ],
   "source": [
    "print(stems)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
