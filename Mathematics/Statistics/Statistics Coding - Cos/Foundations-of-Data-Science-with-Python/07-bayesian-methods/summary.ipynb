{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "225a722c-7b09-47e8-95bb-99d97e4b9e71",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "A great philosopher said:\n",
    "\n",
    "<center>\n",
    "    <font size=+2>\n",
    "    “Beliefs don’t change facts. Facts, if you’re rational, should change your beliefs”     </font> \n",
    "    <br><font size=+1> -- <i><a href=\"https://en.wikipedia.org/wiki/Ricky_Gervais\">Ricky Gervais</a></i> </font>\n",
    "</center>\n",
    "\n",
    "from [Ricky Gervais - The Unbelievers Interview](https://youtu.be/iUUpvrP-gzQ).\n",
    "\n",
    "This is the essence of Bayesian methods. We have some existing beliefs (*a priori* probabilities), but when presented with new facts, evidence, or data, Bayesian methods provide us with a systematic approach to updating our beliefs (*a posteriori* probabilities).\n",
    "\n",
    "We began with deriving the core tool for performing these updates -- Bayes' rule:\n",
    "\n",
    "```{admonition} Bayes' Rule\n",
    ":class: tip\n",
    "For  a stochastic system with a discrete set of possible input events $\\{ A_0, A_i, \\ldots \\}$ and a discrete set of possible output events $\\{B_0, B_1, \\ldots\\}$, the *a posteriori* probabilities $P(A_i|B_j)$ can be written in terms of the likelihoods ($P(B_j|A_i)$) and the *a priori* probabilities ($P(A_i)$) as \n",
    "\n",
    "\n",
    "$$\n",
    "P(A_i|B_j) = \\frac{P\\left(B_j \\left \\vert A_i \\right. \\right) P\\left( A_i \\right)}\n",
    "{ \\sum_i P\\left(B_j \\left \\vert A_i \\right. \\right) P\\left( A_i \\right)}.\n",
    "$$\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc1f8e9-b9bd-4afb-9908-9d3a4d4e5100",
   "metadata": {},
   "source": [
    "We applied Bayes' rule to show that for a 40-year old woman who has a mammogram test come back positive, the probability that she actually has cancer is very small (around 0.01). This is an example of the *Base Rate Fallacy*, in which people tend to focus on the conditional probabilities of events (i.e., the false alarm rate for mammograms is low), while neglecting the base rates (i.e., *a priori* probabilities). In this case, it is very rare for a woman in her 40s to have breast cancer, and this strong *a priori* information results in the *a posteriori* probability of cancer being low, even when cancer has been detected on a mammogram.\n",
    "\n",
    "We introduced the concept of *stochastic systems with hidden state*, in which there is some unknown memory or stored condition internal to the system that cannot be directly observed. Bayesian approaches allow us to update our beliefs about this hidden state after observing outputs from the system.\n",
    "\n",
    "We introduced optimal decision problems, in which a system has an input and an output that depends on that input in some random way (for instance, because of noise). We used a binary-input communication system to illustrate this, and we showed two different approaches to making optimal decisions about the input given the observed outputs:\n",
    "* **Maximum likelihood decision rules** have the advantage that they only require information about the *likelihoods*, which are the conditional probabilities of the outputs given the inputs.\n",
    "* **Maximum *a posteriori* decision rules** are Bayesian approaches that find the most probable input given the observed output; however, they require knowledge about both the likelihoods and the *a priori* probabilities of the inputs.\n",
    "\n",
    "Finally, we briefly introduced Bayesian hypothesis testing and gave a simple example with different types of assumptions on the *a priori* probabilities. In particular, we introduced the concept of an *uninformative prior*, in which every input or system state is assumed equally likely to occur.  We also considered *informative prior*, which uses knowledge outside of the data to choose the *a priori* probabilities. We showed how to conduct both binary tests and nonbinary hypothesis tests. For the latter, we introduced the concepts of *credible intervals*, which are intervals that contain some specified percentage of the *a posteriori* probabilities. A hypothesis can be rejected in a Bayesian framework if it is not within the credible interval.\n",
    "\n",
    "Bayesian approaches are especially powerful when our data does not come from discrete sets. However, we will need new approaches to model and analyze such random phenomena, and in the next chapter we introduce random variables to help fulfill this need."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
