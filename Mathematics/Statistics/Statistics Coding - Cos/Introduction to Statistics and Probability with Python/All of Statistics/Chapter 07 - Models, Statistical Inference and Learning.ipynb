{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Models, Statistical Inference and Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Parametric and Nonparametric Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **statistical model** is a set of distributions $\\mathfrak{F}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A **parametric model** is a set $\\mathfrak{F}$ that may be parametrized by a finite number of parameters.  For example, if we assume that data comes from a normal distribution then\n",
    "\n",
    "$$ \\mathfrak{F} = \\left\\{ f(x; \\mu, \\sigma) = \n",
    "\\frac{1}{\\sigma\\sqrt{\\pi}} \\exp \\left\\{ -\\frac{1}{2\\sigma^2} \\left(x - \\mu \\right)^2 \\right\\}, \\; \\; \n",
    "\\mu \\in \\mathbb{R}, \\; \\sigma > 0\n",
    "\\right\\} $$\n",
    "\n",
    "In general, a parametric model takes the form\n",
    "\n",
    "$$ \\mathfrak{F} = \\left\\{ f(x; \\theta) : \\; \\theta \\in \\Theta \\right\\} $$\n",
    "\n",
    "where $\\theta$ is an unknown parameter that takes values in the **parameter space** $\\Theta$.\n",
    "\n",
    "If $\\theta$ is a vector and we are only interested in one component of $\\theta$, we call the remaining parameters **nuisance parameters**.\n",
    "\n",
    "A **nonparametric model** is a set $\\mathfrak{F}$ that cannot be parametrized by a finite number of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some notation\n",
    "\n",
    "If $\\mathfrak{F} = \\{ f(x; \\theta) : \\; \\theta \\in \\Theta \\}$ is a parametric model, we write\n",
    "\n",
    "$$\\mathbb{P}_\\theta(X \\in A) = \\int_A f(x; \\theta)dx$$\n",
    "\n",
    "$$\\mathbb{E}_\\theta(X \\in A) = \\int_A x f(x; \\theta)dx$$ \n",
    "\n",
    "The subscript $\\theta$ indicates that the probability or expectation is defined with respect to $f(x; \\theta)$; it does not mean we are averaging over $\\theta$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Fundamental Concepts in Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.1 Point estimation\n",
    "\n",
    "Let $X_1, \\dots, X_n$ be $n$ iid data points from some distribution $F$. A point estimator $\\hat{\\theta_n}$ of a parameter $\\theta$ is some function:\n",
    "\n",
    "$$ \\hat{\\theta_n} = g(X_1, \\dots, X_n) $$\n",
    "\n",
    "We define\n",
    "\n",
    "$$ \\text{bias}(\\hat{\\theta_n}) = \\mathbb{E}_\\theta(\\hat{\\theta_n}) - \\theta $$\n",
    "\n",
    "to be the bias of $\\hat{\\theta_n}$.  We say that $\\hat{\\theta_n}$ is **unbiased** if $ \\mathbb{E}_\\theta(\\hat{\\theta_n}) = 0 $.\n",
    "\n",
    "A point estimator $\\hat{\\theta_n}$ of a parameter $\\theta$ is **consistent** if $\\hat{\\theta_n} \\xrightarrow{\\text{P}} \\theta$.\n",
    "\n",
    "The distribution of $\\hat{\\theta_n}$ is called the **sampling distribution**.\n",
    "\n",
    "The standard deviation of $\\hat{\\theta_n}$ is called the **standard error**, denoted by $\\text{se}$:\n",
    "\n",
    "$$ \\text{se} = \\text{se}(\\hat{\\theta_n}) = \\sqrt{\\mathbb{V}(\\hat{\\theta_n})} $$\n",
    "\n",
    "Often it is not possible to compute the standard error but usually we can estimate the standard error.  The estimated standard error is denoted by $\\hat{\\text{se}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**.  Let $X_1, \\dots, X_n \\sim \\text{Bernoulli}(p)$ and let $\\hat{p_n} = n^{-1} \\sum_i X_i$.  Then $\\mathbb{E}(\\hat{p_n}) = n^{-1} \\sum_i \\mathbb{E}(X_i) = p$ so $\\hat{p_n}$ is unbiased.  The standard error is $\\text{se} = \\sqrt{\\mathbb{V}(\\hat{p_n})} = \\sqrt{p(1-p)/n}$.  The estimated standard error is $\\hat{\\text{se}} = \\sqrt{\\hat{p}(1 - \\hat{p})/n}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The quality of a point estimate is sometimes assessed by the **mean squared error**, or MSE, defined by:\n",
    "\n",
    "$$ \\text{MSE} = \\mathbb{E}_\\theta \\left( \\hat{\\theta_n} - \\theta \\right)^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 7.8**.  The MSE can be rewritten as:\n",
    "\n",
    "$$ \\text{MSE} = \\text{bias}(\\hat{\\theta_n})^2 + \\mathbb{V}_\\theta(\\hat{\\theta_n}) $$\n",
    "\n",
    "**Proof**.  Let $\\bar{\\theta_n} = \\mathbb{E}_\\theta(\\hat{\\theta_n})$.  Then\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{E}_\\theta(\\hat{\\theta_n} - \\theta)^2 & = \\mathbb{E}_\\theta(\\hat{\\theta_n} - \\bar{\\theta_n} + \\bar{\\theta_n} - \\theta)^2 \\\\\n",
    "&=  \\mathbb{E}_\\theta(\\hat{\\theta_n} - \\bar{\\theta_n})^2\n",
    "  + 2 (\\bar{\\theta_n} - \\theta) \\mathbb{E}_\\theta(\\hat{\\theta_n} - \\bar{\\theta_n})\n",
    "  + \\mathbb{E}_\\theta(\\hat{\\theta_n} - \\theta)^2 \\\\\n",
    "&= (\\bar{\\theta_n} - \\theta)^2 + \\mathbb{E}_\\theta(\\hat{\\theta_n} - \\bar{\\theta_n})^2 \\\\\n",
    "&= \\text{bias}^2 + \\mathbb{V}_\\theta(\\hat{\\theta_n})\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 7.9**.  If $\\text{bias} \\rightarrow 0$ and $\\text{se} \\rightarrow 0$ as $n \\rightarrow \\infty$ then $\\hat{\\theta_n}$ is consistent, that is, $\\hat{\\theta_n} \\xrightarrow{\\text{P}} \\theta$.\n",
    "\n",
    "**Proof**.  If $\\text{bias} \\rightarrow 0$ and $\\text{se} \\rightarrow 0$ then, by theorem 7.8, $\\text{MSE} \\rightarrow 0$.  It follows that $\\hat{\\theta_n} \\xrightarrow{\\text{qm}} \\theta$ -- and quadratic mean convergence implies probability convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An estimator is **asymptotically Normal** if\n",
    "\n",
    "$$ \\frac{\\hat{\\theta_n} - \\theta}{\\text{se}} \\leadsto N(0, 1) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.2 Confidence sets\n",
    "\n",
    "A $1 - \\alpha$ **confidence interval** for a parameter $\\theta$ is an interval $C_n = (a, b)$ where $a = a(X_1, \\dots, X_n)$ and $b = b(X_1, \\dots, _n)$ are functions of the data such that\n",
    "\n",
    "$$\\mathbb{P}_\\theta(\\theta \\in C_n) \\geq 1 - \\alpha, \\;\\; \\text{for all } \\theta \\in \\Theta$$\n",
    "\n",
    "In words, $(a, b)$ traps $\\theta$ with probability $1 - \\alpha$.  We call $1 - \\alpha$ the **coverage** of the confidence interval.\n",
    "\n",
    "Note:  **$C_n$ is random and $\\theta$ is fixed!**\n",
    "\n",
    "If $\\theta$ is a vector then we use a confidence set (such as a sphere or ellipse) instead of an interval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Point estimators often have a limiting Normal distribution, meaning $\\hat{\\theta_n} \\approx N(\\theta, \\hat{\\text{se}}^2)$.  In this case we can construct (approximate) confidence intervals as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Theorem 7.14 (Normal-based Confidence Interval)**.  Suppose that $\\hat{\\theta_n} \\approx N(\\theta, \\hat{\\text{se}}^2)$.  Let $\\Phi$ be the CDF of a standard Normal and let $z_{\\alpha/2} = \\Phi^{-1}\\left(1 - (\\alpha / 2)\\right)$, that is, $\\mathbb{P}(Z > z_{\\alpha/2}) = \\alpha/2$ and $\\mathbb{P}(-z_{\\alpha/2} < Z < z_{alpha/2}) = 1 - \\alpha$ where $Z \\sim N(0, 1)$.  Let\n",
    "\n",
    "$$ C_n = \\left(\\hat{\\theta_n} - z_{\\alpha/2} \\hat{\\text{se}}, \\; \\hat{\\theta_n} + z_{\\alpha/2} \\hat{\\text{se}}\\right) $$\n",
    "\n",
    "Then\n",
    "\n",
    "$$ \\mathbb{P}_\\theta(\\theta \\in C_n) \\rightarrow 1 - \\alpha $$.\n",
    "\n",
    "**Proof**.\n",
    "\n",
    "Let $Z_n = (\\hat{\\theta_n} - \\theta) / \\hat{\\text{se}}$.  By assumption $Z_n \\leadsto Z \\sim N(0, 1)$.  Hence,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathbb{P}_\\theta(\\theta \\in C_n) \n",
    "& = \\mathbb{P}_\\theta \\left(\\hat{\\theta_n} - z_{\\alpha/2} \\hat{\\text{se}} < \\theta < \\hat{\\theta_n} + z_{\\alpha/2} \\hat{\\text{se}} \\right) \\\\\n",
    "& = \\mathbb{P}_\\theta \\left(-z_{\\alpha/2} < \\frac{\\hat{\\theta_n} - \\theta}{\\hat{\\text{se}}} < z_{\\alpha/2} \\right) \\\\\n",
    "& \\rightarrow \\mathbb{P}\\left(-z_{\\alpha/2} < Z < z_{\\alpha/2}\\right) \\\\\n",
    "&= 1 - \\alpha\n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.3.3 Hypothesis Testing\n",
    "\n",
    "In **hypothesis testing**, we start with some default theory -- called a **null hypothesis** -- and we ask if the data provide sufficient evidence to reject the theory.  If not we retain the null hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 Technical Appendix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Our definition of confidence interval requires that $\\mathbb{P}_\\theta(\\theta \\in C_n) \\geq 1 - \\alpha$ for all $\\theta \\in \\Theta$.\n",
    "- A **pointwise asymptotic** confidence interval requires that $\\lim \\inf_{n \\rightarrow \\infty} \\mathbb{P}_\\theta(\\theta \\in C_n) \\geq 1 - \\alpha$ for all $\\theta \\in \\Theta$.\n",
    "- An **uniform asymptotic** confidence interval requires that $\\lim \\inf_{n \\rightarrow \\infty} \\inf{\\theta \\in \\Theta} \\mathbb{P}_\\theta(\\theta \\in C_n) \\geq 1 - \\alpha$.\n",
    "\n",
    "The approximate Normal-based interval is a pointwise asymptotic confidence interval.  In general, it might not be a uniform asymptotic confidence interval."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
