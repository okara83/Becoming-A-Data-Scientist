{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyTorch Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will explain how the auto-differentiation module of PyTorch works. \n",
    "This module is named **Autograd**.\n",
    "\n",
    "We will first present you how you can comppute gradient using PyTorch for a specific variable and how to check the value of the gradient. Then we will use the **backward** function to do the gradient computation. Finally, we will see how to detach a tensor from its computation history and how to tell PyTorch not to keep track of the operations (useful in inference!).\n",
    "\n",
    "More advanced autograd functions are also explained, but only glanced over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How does Autograd work?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you do operations on Tensors, PyTorch can keep track of the computation graph in order to be able to backpropagate.\n",
    "To tell PyTorch to record operations performed on a tensor, each tensor has a function called **`requires_grad_`**.\n",
    "\n",
    "If there’s at least one input to an operation that requires gradient, its output will also require gradient. Conversely, only if all inputs don’t require gradient, the output also won’t require it. Backward computation is never performed in the subgraphs, where all Tensors didn’t require gradients.\n",
    "\n",
    "Inplace operations are non-differentiable. That is why `x.zero_()` gives an error if x requires gradient computation.\n",
    "\n",
    "For a tensor x, the underlying data is stored in a tensor that is accessible via **x.data**. If you do an operation on x.data PyTorch does not add the operation to the computation graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function requires_grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tensor has a property **`requires_grad`** specifying whether the gradient should be computed during backward pass.\n",
    "\n",
    "The function **`requires_grad_(bool)`** (notice the trailing **\\_** ) is used to change this property. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A :  tensor([[8., 7.]])\n",
      "A.requires_grad : False\n",
      "A.requires_grad : True\n",
      "A.requires_grad : False\n"
     ]
    }
   ],
   "source": [
    "A = torch.randint(10, (1,2), dtype=torch.float)\n",
    "print(\"A : \", A)\n",
    "\n",
    "print(\"A.requires_grad :\", A.requires_grad)\n",
    "\n",
    "A.requires_grad_(True)\n",
    "print(\"A.requires_grad :\", A.requires_grad)\n",
    "\n",
    "A.requires_grad_(False)\n",
    "print(\"A.requires_grad :\", A.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Backward function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will see a simple example of how to compute the gradient of a function automatically with pytorch.\n",
    "We will check that it correspond to what we can compute manually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's look at the function $f(x, y) = \\sin\\big( \\langle x , y \\rangle \\big)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f = tensor(0.2964, grad_fn=<SinBackward>)\n"
     ]
    }
   ],
   "source": [
    "X = torch.Tensor([1, 2, 3]).requires_grad_(True)\n",
    "Y = torch.Tensor([5, 6, 7]).requires_grad_(True)\n",
    "\n",
    "f = torch.sin(torch.dot(X,Y))\n",
    "print(\"f =\", f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We simply need to call the `__backward__` function on $f$\n",
    "\n",
    "### The `__backward__` function will automatically compute all the gradients of $f$ with respect to the inputs using the chain rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NOTE: \n",
    "#### Gradient is populated by the backward function\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/sd/1vc_q83x5rn9jjrd0x47_cc00000gn/T/ipykernel_92749/555863162.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X.grad :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Y.grad :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\n-- Backward --\\n\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"X.grad :\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"X.grad :\", X.grad)\n",
    "print(\"Y.grad :\", Y.grad)\n",
    "f.backward()\n",
    "print(\"\\n-- Backward --\\n\")\n",
    "print(\"X.grad :\", X.grad)\n",
    "print(\"Y.grad :\", Y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Computing Manually:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### $f$ can be written as a composite function $f = h \\circ g$\n",
    "\n",
    "###  $h(z) = \\sin(z)$ with derivative $\\dfrac{d h}{d z}(z) = \\cos(z)$\n",
    "\n",
    "###  $g(x, y) =  \\langle x , y \\rangle$ with partial derivatives $\\dfrac{\\partial g}{\\partial x}(x, y) = y$ and $\\dfrac{\\partial g}{\\partial y}(x, y) = x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Using the chain rule, we can easily get the derivative of $f$ with respect to $x$ and $y$:\n",
    "\n",
    "###  $\\dfrac{d f }{d x} (x,y) = \\cos\\big( \\langle x , y \\rangle \\big) \\cdot y $\n",
    "\n",
    "  and\n",
    "\n",
    "###  $\\dfrac{d f }{d y} (x,y) = \\cos\\big( \\langle x , y \\rangle \\big) \\cdot x $\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df / dx =  tensor([4.7754, 5.7304, 6.6855], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dfdx = torch.cos(torch.dot(X,Y)) * Y\n",
    "print(\"df / dx = \", dfdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df / dy =  tensor([0.9551, 1.9101, 2.8652], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "dfdy = torch.cos(torch.dot(X,Y)) * X\n",
    "print(\"df / dy = \", dfdy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Leaf Variable "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variable that __was created by the user__ and was therefore not the result of _any_ operation is called a **leaf variable**.  \n",
    "All variables that have the __`requires_grad` property to False__ are also considered as **leaf variable**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.is_leaf : True\n",
      "B.is_leaf : False\n",
      "C.is_leaf : False\n"
     ]
    }
   ],
   "source": [
    "A = torch.Tensor([[1, 2], [3, 4]]).requires_grad_()\n",
    "B = torch.Tensor([[1, 2], [3, 4]]).requires_grad_() + 2  # B is the result of an operation (+)\n",
    "C = 5 * A  # C is the result of an operation (*)\n",
    "print(\"A.is_leaf :\", A.is_leaf)\n",
    "print(\"B.is_leaf :\", B.is_leaf)\n",
    "print(\"C.is_leaf :\", C.is_leaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Detach function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A variable can have a long computation history, but you may want to consider it as a __new leaf variable__ without history.\n",
    "\n",
    "For that, you can use the `detach` function, which detaches the tensor from its history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "B :  tensor(0.3132)\n",
      "B.grad_fn : None\n",
      "B.is_leaf : True\n",
      "\n",
      " -- B.detach_() -- \n",
      "\n",
      "B :  tensor(0.3132)\n",
      "B.grad_fn : None\n",
      "B.is_leaf : True\n"
     ]
    }
   ],
   "source": [
    "A = torch.rand(1,2)\n",
    "B = A.mean()\n",
    "\n",
    "print(\"B : \", B)\n",
    "print(\"B.grad_fn :\", B.grad_fn)\n",
    "print(\"B.is_leaf :\", B.is_leaf)\n",
    "\n",
    "B.detach_()\n",
    "print(\"\\n -- B.detach_() -- \\n\")\n",
    "\n",
    "print(\"B : \", B)\n",
    "print(\"B.grad_fn :\", B.grad_fn)\n",
    "print(\"B.is_leaf :\", B.is_leaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This won't work since B has no history.\n",
    "# B.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No_grad function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At inference time, you don't want Pytorch to build a computation graph. \n",
    "This can be achieved by wrapping your inference code into the __`with torch.no_grad()`__ context manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x.requires_grad :  True\n",
      "y.requires_grad :  True\n",
      "y.requires_grad :  False\n"
     ]
    }
   ],
   "source": [
    "x = torch.randn(3, requires_grad=True)\n",
    "print(\"x.requires_grad : \", x.requires_grad)\n",
    "\n",
    "y = (x ** 2)\n",
    "print(\"y.requires_grad : \", y.requires_grad)\n",
    "\n",
    "with torch.no_grad():\n",
    "    y = (x ** 2)\n",
    "    print(\"y.requires_grad : \", y.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note: Autograd in previous PyTorch versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In older versions of PyTorch, one had to wrap a `Tensor` into a Autograd object called `Variable`.\n",
    "\n",
    "`Variable` was a thin wrapper around a `Tensor` object, that also held the gradient w.r.t. to it, and a reference to a function that created it. This reference allowed retracing the whole chain of operations that created the data.\n",
    "\n",
    "**Now, `Tensors` are by default `Variable` and we don't need to worry about this anymore**, but you may still encounter it in some \"old\" code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-2.8454, -0.3651,  0.7429,  0.9934, -0.9995],\n",
       "        [-1.3726, -0.7660, -0.3420,  0.5346, -0.1022],\n",
       "        [-0.4675,  1.7554,  0.1196,  1.4556,  0.6999],\n",
       "        [-0.1888,  0.2787, -2.1573,  0.1700, -0.3651],\n",
       "        [-1.2949,  0.4398, -0.2459,  1.1139,  0.8986]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.autograd import Variable\n",
    "\n",
    "x = Variable(torch.randn(5, 5))\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced concepts of Autograd "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following concepts are more advanced and may want to skip it for now.  \n",
    "We won't go through them, but there are here for you to come back to later when you feel more comfortable with pytorch.  \n",
    "You can also check the [Pytorch Doc](https://pytorch.org/docs/stable/autograd.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retain Grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When doing the backward pass, Autograd computes the gradient of the output with respect to every intermediate variables. However, by default, only gradients of variables that were **created by the user** (leaf) and have the __`requires_grad` property to True__ are saved.\n",
    "\n",
    "Indeed, most of the time when training a model you only need the gradient of a loss w.r.t. to your model parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.grad : None\n",
      "B.grad : None\n",
      "\n",
      "-- Backward --\n",
      "\n",
      "A.grad : tensor([[1.2500, 1.2500],\n",
      "        [1.2500, 1.2500]])\n",
      "B.grad : None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sd/1vc_q83x5rn9jjrd0x47_cc00000gn/T/ipykernel_35089/3750451614.py:8: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  print(\"B.grad :\", B.grad)\n",
      "/var/folders/sd/1vc_q83x5rn9jjrd0x47_cc00000gn/T/ipykernel_35089/3750451614.py:12: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  print(\"B.grad :\", B.grad)\n"
     ]
    }
   ],
   "source": [
    "A = torch.Tensor([[1, 2], [3, 4]])\n",
    "A.requires_grad_()\n",
    "\n",
    "B = 5 * (A + 3)\n",
    "C = B.mean()\n",
    "\n",
    "print(\"A.grad :\", A.grad)\n",
    "print(\"B.grad :\", B.grad)\n",
    "C.backward()\n",
    "print(\"\\n-- Backward --\\n\")\n",
    "print(\"A.grad :\", A.grad)\n",
    "print(\"B.grad :\", B.grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.grad : None\n",
      "B.grad : None\n",
      "\n",
      "-- Backward --\n",
      "\n",
      "A.grad : tensor([[1.2500, 1.2500],\n",
      "        [1.2500, 1.2500]])\n",
      "B.grad : tensor([[0.2500, 0.2500],\n",
      "        [0.2500, 0.2500]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.Tensor([[1, 2], [3, 4]])\n",
    "A.requires_grad_()\n",
    "\n",
    "B = 5 * (A + 3)\n",
    "B.retain_grad()  # <----- This line let us have access to gradient wrt. B after the backward pass\n",
    "C = B.mean()\n",
    "\n",
    "\n",
    "print(\"A.grad :\", A.grad)\n",
    "print(\"B.grad :\", B.grad)\n",
    "C.backward()\n",
    "print(\"\\n-- Backward --\\n\")\n",
    "print(\"A.grad :\", A.grad)\n",
    "print(\"B.grad :\", B.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient accumulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can backward a first time and get a gradient for A, then do some other computation using A and then backward again.  \n",
    "Gradients will get accumulated in A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A.grad : None\n",
      "\n",
      "-- Backward --\n",
      "\n",
      "A.grad : tensor([[1.2500, 1.2500],\n",
      "        [1.2500, 1.2500]])\n",
      "\n",
      "-- Backward --\n",
      "\n",
      "A.grad : tensor([[2.5000, 2.5000],\n",
      "        [2.5000, 2.5000]])\n"
     ]
    }
   ],
   "source": [
    "A = torch.Tensor([[1, 2], [3, 4]]).requires_grad_()\n",
    "\n",
    "print(\"A.grad :\", A.grad)\n",
    "\n",
    "B = 5 * (A + 3)\n",
    "C = B.mean()\n",
    "C.backward()\n",
    "\n",
    "print(\"\\n-- Backward --\\n\")\n",
    "print(\"A.grad :\", A.grad)\n",
    "\n",
    "B = 5 * (A + 3)\n",
    "C = B.mean()\n",
    "C.backward()\n",
    "\n",
    "print(\"\\n-- Backward --\\n\")\n",
    "print(\"A.grad :\", A.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Happening When We Call These Methods?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part is to give a glimpse of how it works under the hood. We don't need to do such inspection in practice.  \n",
    "Here, we have a look at the computation graph that autograd builds on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.Tensor([[1, 2], [3, 4]])\n",
    "A.requires_grad_()\n",
    "\n",
    "B = 5 * (A + A)\n",
    "C = B.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each tensor has a gradient function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "<MulBackward0 object at 0x110673280>\n",
      "<MeanBackward0 object at 0x111f92100>\n"
     ]
    }
   ],
   "source": [
    "print(A.grad_fn)\n",
    "print(B.grad_fn)\n",
    "print(C.grad_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also \"walk\" on the computation graph by calling the `next_functions` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MeanBackward0 object at 0x111f69dc0>\n",
      "((<MulBackward0 object at 0x111f69f40>, 0),)\n",
      "((<AddBackward0 object at 0x111f69dc0>, 0), (None, 0))\n",
      "((<AccumulateGrad object at 0x111f69f40>, 0), (<AccumulateGrad object at 0x111f69f40>, 0))\n"
     ]
    }
   ],
   "source": [
    "grad_fn = C.grad_fn\n",
    "print(grad_fn)\n",
    "\n",
    "grad_fn = grad_fn.next_functions\n",
    "print(grad_fn)\n",
    "\n",
    "grad_fn = grad_fn[0][0].next_functions\n",
    "print(grad_fn)\n",
    "\n",
    "grad_fn = grad_fn[0][0].next_functions\n",
    "print(grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
