{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalEqn(X,y):\n",
    "    \"\"\" Computes the closed-form solution to linear regression\"\"\"\n",
    "    theta = np.linalg.inv(X.T.dot(X)).dot(X.T).dot(y)\n",
    "    return theta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Note:\n",
    "\n",
    "1) The reason why we would use Gradient Descent in Linear Regression is because $\\textbf{it’s computationally cheaper to find optima}$.\n",
    "\n",
    "2) The normal equations has the matrix inversion in it and inverting a matrix is an expensive operation. The design matrix $\\boldsymbol{X}$ has $\\textbf{k+1}$ columns where $\\textbf{k}$ is the number of predictors ($x^{1}$, $x^{2}$, $x^{3}$,…) and $\\textbf{m rows}$ of samples. If the number of samples < 100, then it is quicker to use the normal equations than Gradient Descent. But, in real life, $\\textbf{k}$ is easily greater than 1,000 and sample size will be greater than 100k. Since the matrix inversion is (O($n^{3}$)), inverting X'X (1,000 by 1,000 matrix) will take a while to calculate.\n",
    "\n",
    "# Summary\n",
    "When we use `Gradient Descent`, `we have to scale the data`. When we use `normal equation`, `we don’t have to`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
