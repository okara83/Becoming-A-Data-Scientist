{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cooperative-asthma"
   },
   "source": [
    "# Resilient Distributed Datasets (RDDs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "external-federation"
   },
   "source": [
    "Spark revolves around the concept of a resilient distributed dataset (RDD), which is a<b> fault-tolerant collection of elements that can be operated on in parallel.</b> <br>There are two ways to create RDDs: <ul><li>parallelizing an existing collection in your driver program</li>, <li> referencing a dataset in an external storage system, such as a shared filesystem, HDFS, HBase, or any data source offering a Hadoop InputFormat.</li></ul>\n",
    "\n",
    "<br><br>\n",
    "<b>Parallelized collections</b> are created by calling SparkContext’s parallelize method on an existing iterable or collection in your driver program. The elements of the collection are copied to form a distributed dataset that can be operated on in parallel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hearing-belgium"
   },
   "source": [
    "For example, here is how to create a parallelized collection holding the numbers 1 to 5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 145,
     "status": "ok",
     "timestamp": 1623131383624,
     "user": {
      "displayName": "Onur Kara",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhD8yfuPKn7D2kG6KoGSsy_qZkAaPpq0fZB2mM7QQ=s64",
      "userId": "04428748629895168234"
     },
     "user_tz": 240
    },
    "id": "Lk3FQOWCKwHG"
   },
   "outputs": [],
   "source": [
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StringType\n",
    "from pyspark import SQLContext\n",
    "from itertools import islice\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 198
    },
    "executionInfo": {
     "elapsed": 9,
     "status": "error",
     "timestamp": 1623131384763,
     "user": {
      "displayName": "Onur Kara",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GhD8yfuPKn7D2kG6KoGSsy_qZkAaPpq0fZB2mM7QQ=s64",
      "userId": "04428748629895168234"
     },
     "user_tz": 240
    },
    "id": "copyrighted-passenger",
    "outputId": "64d1656c-0f6f-4a9e-d2dc-e28d11405c53"
   },
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "distData = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "responsible-frost"
   },
   "source": [
    "Once created, the distributed dataset (distData) can be operated on in parallel. <br>For example, we can call distData.reduce(lambda a, b: a + b) to add up the elements of the list;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "complimentary-shadow",
    "outputId": "9ef2e783-76e4-4ddb-97f8-ab50da5dd655"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distData.reduce(lambda a, b: a + b) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "provincial-error"
   },
   "source": [
    "# External Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flush-diameter"
   },
   "source": [
    "PySpark can create distributed datasets from any storage source supported by Hadoop, including your local file system, HDFS, Cassandra, HBase, Amazon S3, etc. Spark supports text files, SequenceFiles, and any other Hadoop InputFormat."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "controlling-lemon"
   },
   "source": [
    "Text file RDDs can be created using SparkContext’s textFile method. This method takes an URI for the file (either a local path on the machine, or a hdfs://, s3n://, etc URI) and reads it as a collection of lines. Below is an example invocation using sc that's loaded with spark by default"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "surprising-benefit"
   },
   "outputs": [],
   "source": [
    "file = sc.textFile(\"file.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "subjective-dance"
   },
   "source": [
    "<b>NOTE:</b> <br>\n",
    "   The above code defines a base RDD from an external file. This dataset is not loaded in memory or otherwise acted on: lines is merely a pointer to the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "earned-prompt",
    "outputId": "ab362355-47cf-4f00-8010-692a869a2beb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<?xml version=\\\\\"1.0\\\\\" encoding=\\\\\"utf-8\\\\\"?>',\n",
       " '<rss version=\\\\\"2.0\\\\\" xmlns:content=\\\\\"http://purl.org/rss/1.0/modules/content/\\\\\" xmlns:dc=\\\\\"http://purl.org/dc/elements/1.1/\\\\\" xmlns:media=\\\\\"http://search.yahoo.com/mrss/\\\\\" xmlns:snf=\\\\\"http://www.smartnews.be/snf\\\\\">',\n",
       " '<channel>',\n",
       " '<title>Kyodo News</title>',\n",
       " '<link>https://this.kiji.is/-/units/288215928582407265</link>',\n",
       " '<description>Kyodo News</description>',\n",
       " '<pubDate>Tue, 19 Nov 2019 15:55:37 +0000</pubDate>',\n",
       " '<language>en</language>',\n",
       " '<copyright>(c) 一般社団法人共同通信社</copyright>',\n",
       " '<ttl>1</ttl>']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.take(10) #take the first 10 lines of that file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "necessary-hollow"
   },
   "source": [
    "# <b>Count all the words in this text file.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "textile-stockholm"
   },
   "outputs": [],
   "source": [
    "counts = file.flatMap(lambda line:line.split(\" \")).map(lambda word: (word,1)).reduceByKey(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sexual-writer"
   },
   "source": [
    "To view the result, again  <span style=\"color:green\">use take! </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "documentary-korea",
    "outputId": "43705f48-f78c-4ad7-a28d-c7327a833f5c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('<?xml', 1),\n",
       " ('version=\\\\\"1.0\\\\\"', 1),\n",
       " ('<rss', 1),\n",
       " ('version=\\\\\"2.0\\\\\"', 1),\n",
       " ('xmlns:content=\\\\\"http://purl.org/rss/1.0/modules/content/\\\\\"', 1),\n",
       " ('xmlns:dc=\\\\\"http://purl.org/dc/elements/1.1/\\\\\"', 1),\n",
       " ('xmlns:snf=\\\\\"http://www.smartnews.be/snf\\\\\">', 1),\n",
       " ('<channel>', 1),\n",
       " ('<title>Kyodo', 1),\n",
       " ('News</title>', 1),\n",
       " ('<link>https://this.kiji.is/-/units/288215928582407265</link>', 1),\n",
       " ('<description>Kyodo', 1),\n",
       " ('19', 28),\n",
       " ('2019', 50),\n",
       " ('<language>en</language>', 1),\n",
       " ('<copyright>(c)', 1),\n",
       " ('一般社団法人共同通信社</copyright>', 1),\n",
       " ('<ttl>1</ttl>', 1),\n",
       " ('', 7205),\n",
       " ('Laos,', 4),\n",
       " ('Thailand,', 3),\n",
       " ('Vietnam', 4),\n",
       " ('warn</title>', 1),\n",
       " ('<guid>http://this.kiji.is/569551319185638497</guid>', 1),\n",
       " ('Severe', 1),\n",
       " ('extreme', 2),\n",
       " ('drought', 6),\n",
       " ('is', 84),\n",
       " ('four', 6),\n",
       " ('lower', 22),\n",
       " ('Basin', 2),\n",
       " ('countries', 3),\n",
       " ('of', 257),\n",
       " ('Cambodia...', 1),\n",
       " ('<pubDate>Wed,', 2),\n",
       " ('20', 6),\n",
       " ('class=\\\\\"ma__p\\\\\">Severe', 1),\n",
       " ('Cambodia,', 2),\n",
       " ('Thailand', 6),\n",
       " ('now', 11)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.take(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "attached-finding"
   },
   "source": [
    "Once created, distFile can be acted on by dataset operations. For example, we can add up the sizes of all the lines using the map and reduce operations as follows: distFile.map(lambda s: len(s)).reduce(lambda a, b: a + b).\n",
    "\n",
    "Some notes on reading files with Spark:\n",
    "\n",
    "If using a path on the local filesystem, the file must also be accessible at the same path on worker nodes. Either copy the file to all workers or use a network-mounted shared file system.\n",
    "\n",
    "All of Spark’s file-based input methods, including textFile, support running on directories, compressed files, and wildcards as well. For example, you can use textFile(\"/my/directory\"), textFile(\"/my/directory/*.txt\"), and textFile(\"/my/directory/*.gz\").\n",
    "\n",
    "The textFile method also takes an optional second argument for controlling the number of partitions of the file. By default, Spark creates one partition for each block of the file (blocks being 128MB by default in HDFS), but you can also ask for a higher number of partitions by passing a larger value. Note that you cannot have fewer partitions than blocks.\n",
    "\n",
    "Apart from text files, Spark’s Python API also supports several other data formats:\n",
    "\n",
    "SparkContext.wholeTextFiles lets you read a directory containing multiple small text files, and returns each of them as (filename, content) pairs. This is in contrast with textFile, which would return one record per line in each file.\n",
    "\n",
    "RDD.saveAsPickleFile and SparkContext.pickleFile support saving an RDD in a simple format consisting of pickled Python objects. Batching is used on pickle serialization, with default batch size 10.\n",
    "\n",
    "SequenceFile and Hadoop Input/Output Formats\n",
    "\n",
    "Note this feature is currently marked Experimental and is intended for advanced users. It may be replaced in future with read/write support based on Spark SQL, in which case Spark SQL is the preferred approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "inner-bargain"
   },
   "source": [
    "# SparkContext Example – PySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amino-carroll"
   },
   "source": [
    "For the 1st example we'll count the number of lines with words 'news' or 'Kyodo' in file.txt. So let's say if there are 5 lines in a file and 3 lines have the word 'news', then the output will be:<br> $\\rightarrow$ Line with 'news': 3. <br>likewise for 'Kyodo'<br><br>\n",
    "<b>Note − We are not creating any SparkContext object in the following example because by default, <span style=\"color:blue\">Spark automatically creates the <span style=\"color:green\">SparkContext object</span> named<span> <span style=\"color:green\"> sc,</span>  when PySpark shell starts.</span>.</span> <br><br> In case you try to create another SparkContext object, you will get the following error  – <span style=\"color:red\"> \"ValueError: Cannot run multiple SparkContexts at once\".</b></span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "exterior-crest",
    "outputId": "f31595bd-e450-4ea0-b160-097ee7d69900"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file.txt MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "ambient-provincial",
    "outputId": "360b68bd-64e4-4a5d-d878-b75d82d87f5a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "file.txt MapPartitionsRDD[3] at textFile at NativeMethodAccessorImpl.java:0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "binary-minute"
   },
   "outputs": [],
   "source": [
    "numAs = file.filter(lambda s: 'news' in s).count()\n",
    "numBs = file.filter(lambda s: 'Kyodo' in s).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "junior-security",
    "outputId": "9ec45580-03e7-42e9-9bf8-7f15de2f7aa4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines with a: 6,\n",
      " Lines with b: 4\n"
     ]
    }
   ],
   "source": [
    "print(\"Lines with a: %s Lines with b: %s\" % (str(numAs)+\",\\n\",str(numBs)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "supported-hearing"
   },
   "source": [
    "# Counting elements in RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "another-funeral"
   },
   "outputs": [],
   "source": [
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "written-sympathy"
   },
   "outputs": [],
   "source": [
    "counts = words.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "acute-poker",
    "outputId": "53a31783-ceec-44e5-809b-2733be773ff9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of elements in RDD: 8\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of elements in RDD: %i\" % (counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "twelve-crisis"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "assured-junior"
   },
   "outputs": [],
   "source": [
    "lineLengths = file.map(lambda s: len(s))\n",
    "totalLength = lineLengths.reduce(lambda a, b: a + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "adapted-christopher"
   },
   "source": [
    "NOTE: <br>\n",
    "    In the above code the first line defines a base RDD from an external file. This dataset is not loaded in memory or otherwise acted on: lines is merely a pointer to the file. <b>The second line defines lineLengths as the result of a map transformation. Again, lineLengths is not immediately computed, due to laziness. </b><span style=\"color:green\">Finally, we run reduce, which is an action.</span> <span style=\"color:red\">At this point Spark breaks the computation into tasks to run on separate machines, and each machine runs both its part of the map and a local reduction, returning only its answer to the driver program.</span>\n",
    "\n",
    "If we also wanted to use lineLengths again later, we could add: lineLengths.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "organizational-dryer",
    "outputId": "2c591e83-145f-4269-acbb-f19aab4b8623"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[15] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lineLengths.persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "furnished-failing"
   },
   "source": [
    "# Using the \"Collect\" method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "increasing-johns"
   },
   "source": [
    "<u>Collect() – Retrieve data from Spark RDD/DataFrame</u>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fantastic-algorithm"
   },
   "source": [
    "Spark collect() and collectAsList() are action operation that is used to retrieve all the elements of the RDD/DataFrame/Dataset (from all nodes) to the driver node. We should use the collect() on smaller dataset usually after filter(), group(), count() e.t.c. Retrieving on larger dataset results in out of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "commercial-garbage",
    "outputId": "959e71a7-6155-4031-9148-937cb6124928"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elements in RDD: ['scala', 'java', 'hadoop', 'spark', 'akka', 'spark vs hadoop', 'pyspark', 'pyspark and spark']\n"
     ]
    }
   ],
   "source": [
    "words = sc.parallelize (\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "coll = words.collect()\n",
    "print(\"Elements in RDD: %s\" % (coll))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "german-variety"
   },
   "source": [
    "# Using <b>\"Filter\"</b><br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "experimental-rehabilitation"
   },
   "source": [
    "A new RDD is returned containing the elements, which satisfies the function inside the filter<br>\n",
    "in example below, filter out the strings not containing \"spark\";"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "covered-jewel",
    "outputId": "4e0e02f3-485c-4342-ca94-dee0db99c585"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitered RDD: ['spark', 'spark vs hadoop', 'pyspark', 'pyspark and spark']\n"
     ]
    }
   ],
   "source": [
    "words = sc.parallelize(\n",
    "   [\"scala\", \n",
    "   \"java\", \n",
    "   \"hadoop\", \n",
    "   \"spark\", \n",
    "   \"akka\",\n",
    "   \"spark vs hadoop\", \n",
    "   \"pyspark\",\n",
    "   \"pyspark and spark\"]\n",
    ")\n",
    "words_filter = words.filter(lambda x: 'spark' in x)\n",
    "filtered = words_filter.collect()\n",
    "print(\"Fitered RDD: %s\" % (filtered))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "duplicate-battery"
   },
   "source": [
    "# Saving and Loading SequenceFiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "consecutive-merchandise"
   },
   "outputs": [],
   "source": [
    "rdd = sc.parallelize(range(1, 4)).map(lambda x: (x, \"a\" * x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "analyzed-selling"
   },
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.saveAsSequenceFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/Users/okara/Downloads/PySpark_for_mentees/myNewSeqFile already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:289)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.api.python.PythonRDD$.saveAsHadoopFile(PythonRDD.scala:604)\n\tat org.apache.spark.api.python.PythonRDD$.saveAsSequenceFile(PythonRDD.scala:573)\n\tat org.apache.spark.api.python.PythonRDD.saveAsSequenceFile(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-2f745d37f31a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaveAsSequenceFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"myNewSeqFile\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m### save sequence to file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/spark-3.0.1/python/pyspark/rdd.py\u001b[0m in \u001b[0;36msaveAsSequenceFile\u001b[0;34m(self, path, compressionCodecClass)\u001b[0m\n\u001b[1;32m   1584\u001b[0m         \"\"\"\n\u001b[1;32m   1585\u001b[0m         \u001b[0mpickledRDD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pickled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1586\u001b[0;31m         self.ctx._jvm.PythonRDD.saveAsSequenceFile(pickledRDD._jrdd, True,\n\u001b[0m\u001b[1;32m   1587\u001b[0m                                                    path, compressionCodecClass)\n\u001b[1;32m   1588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.1/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1304\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1306\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.1/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    126\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/spark-3.0.1/python/lib/py4j-0.10.9-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.saveAsSequenceFile.\n: org.apache.hadoop.mapred.FileAlreadyExistsException: Output directory file:/Users/okara/Downloads/PySpark_for_mentees/myNewSeqFile already exists\n\tat org.apache.hadoop.mapred.FileOutputFormat.checkOutputSpecs(FileOutputFormat.java:131)\n\tat org.apache.spark.internal.io.HadoopMapRedWriteConfigUtil.assertConf(SparkHadoopWriter.scala:289)\n\tat org.apache.spark.internal.io.SparkHadoopWriter$.write(SparkHadoopWriter.scala:71)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopDataset$1(PairRDDFunctions.scala:1090)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopDataset(PairRDDFunctions.scala:1088)\n\tat org.apache.spark.rdd.PairRDDFunctions.$anonfun$saveAsHadoopFile$4(PairRDDFunctions.scala:1061)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:388)\n\tat org.apache.spark.rdd.PairRDDFunctions.saveAsHadoopFile(PairRDDFunctions.scala:1026)\n\tat org.apache.spark.api.python.PythonRDD$.saveAsHadoopFile(PythonRDD.scala:604)\n\tat org.apache.spark.api.python.PythonRDD$.saveAsSequenceFile(PythonRDD.scala:573)\n\tat org.apache.spark.api.python.PythonRDD.saveAsSequenceFile(PythonRDD.scala)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:64)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:564)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.base/java.lang.Thread.run(Thread.java:832)\n"
     ]
    }
   ],
   "source": [
    "rdd.saveAsSequenceFile(\"myNewSeqFile\") ### save sequence to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "critical-divide",
    "outputId": "cbdc1101-b43a-43c6-fb78-5d0d44a7fc0b"
   },
   "outputs": [],
   "source": [
    "sorted(sc.sequenceFile(\"myNewSeqFile\").collect()) ### load sequence from file and collect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "informative-appeal"
   },
   "source": [
    "# Spark's API relies heavily on passing functions in the driver program to run on the cluster. There are three recommended ways to do this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "transparent-financing"
   },
   "source": [
    "\n",
    "<ul>\n",
    "<li>Lambda expressions, for simple functions that can be written as an expression. (Lambdas do not support multi-statement functions or statements that do not return a value.)</li>\n",
    "<li>Local defs inside the function calling into Spark, for longer code.</li>\n",
    "<li>Top-level functions in a module.</li>\n",
    "</ul><br>\n",
    "For example, to pass a longer function than can be supported using a lambda, consider the code below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "surrounded-papua"
   },
   "outputs": [],
   "source": [
    "\"\"\"MyScript.py\"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "    def myFunc(s):\n",
    "        words = s.split(\" \")\n",
    "        return len(words)\n",
    "\n",
    "    sc.textFile(\"filenew.txt\").map(myFunc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "excellent-chambers"
   },
   "source": [
    "Note that while it is also possible to pass a reference to a method in a class instance (as opposed to a singleton object), this requires sending the object that contains that class along with the method. For example, consider:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "understood-present"
   },
   "outputs": [],
   "source": [
    "class MyClass(object):\n",
    "    def func(self, s):\n",
    "        return s\n",
    "    def doStuff(self, rdd):\n",
    "        return rdd.map(self.func)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "painful-pollution"
   },
   "source": [
    "Here, if we create a new MyClass and call doStuff on it, the map inside there references the func method of that MyClass instance, so the whole object needs to be sent to the cluster.\n",
    "\n",
    "In a similar way, accessing fields of the outer object will reference the whole object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cross-stanford"
   },
   "outputs": [],
   "source": [
    "class MyClass(object):\n",
    "    def __init__(self):\n",
    "        self.field = \"Hello\"\n",
    "    def doStuff(self, rdd):\n",
    "        return rdd.map(lambda s: self.field + s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "italic-planet"
   },
   "source": [
    "To avoid this issue, the simplest way is to copy field into a local variable instead of accessing it externally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hundred-brief"
   },
   "outputs": [],
   "source": [
    "def doStuff(self, rdd):\n",
    "    field = self.field\n",
    "    return rdd.map(lambda s: field + s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tropical-berry"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "senior-australian"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "royal-collectible"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "pyspark_tutorial_1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
