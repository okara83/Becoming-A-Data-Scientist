{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Mode Automatic Differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the heart of modern machine learning, so popular in 2019, is an optimization problem. Optimization means gradients, so suddenly differentiation, especially automatic differentiation (AD), is exciting.\n",
    "\n",
    "The first time one hears about AD, it is easy to imagine what it is. Surely it is straightforward symbolic differentiation applied to code. One imagines automatically doing what is learned in a calculus class, like\n",
    "$$ \\frac{d}{dx}x^n = n x^{n-1}. $$\n",
    "**This is not how AD works.**\n",
    "\n",
    "Ok, then it must be numerical differentiation and to first order\n",
    "$$ \\frac{df}{dx} \\approx \\frac{f(x+h) - f(x)}{\\Delta h} $$\n",
    "**This is also not how AD works.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Babylonian sqrt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Repeat $t \\leftarrow (t + x/2)/2$ until $t$ converges to $\\sqrt{x}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@inline function Babylonian(x; N = 10)\n",
    "    t = (1+x)/2\n",
    "    for i = 2:N\n",
    "        t = (t + x/t)/2\n",
    "    end\n",
    "    t\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "α = π\n",
    "\n",
    "Babylonian(α), √α"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Babylonian(2), √2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using PyPlot\n",
    "\n",
    "xs = 0:0.01:49\n",
    "\n",
    "for i in 1:5\n",
    "    plot(xs, [Babylonian(x; N=i) for x in xs], label=\"Iteration $i\")\n",
    "end\n",
    "\n",
    "plot(xs, sqrt.(xs), label=\"sqrt\", color=\"black\")\n",
    "\n",
    "legend()\n",
    "title(\"Those Babylonians really knew how to √\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ... and now the derivative, almost by magic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ten lines of Julia code! No mention of 1/2 over sqrt(x). D for \"dual number\", invented by Clifford in 1873."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct D <: Number\n",
    "    x::Float64 # value\n",
    "    ϵ::Float64 # derivative\n",
    "end\n",
    "\n",
    "import Base: +, /, convert, promote_rule\n",
    "a::D + b::D = D(a.x + b.x, a.ϵ + b.ϵ) # sum rule\n",
    "a::D / b::D = D(a.x / b.x, (b.x * a.ϵ - a.x * b.ϵ)/b.x^2) # quotient rule\n",
    "convert(::Type{D}, x::Real) = D(x, zero(x))\n",
    "promote_rule(::Type{D}, ::Type{<:Number}) = D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same babylonian algorithm with no rewrite at all computes properly the derivative as the check shows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Babylonian(D(5, 1)) |> dump"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = 5; Babylonian(D(x, 1)), (√x, 0.5 / √x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = π; Babylonian(D(x, 1)), (√x, 0.5 / √x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It just works!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "code_llvm(Babylonian, (D,); debuginfo=:none)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Symbolically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may be of some value to understand that the below is mathematically equivalent, though not what the computation is doing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SymPy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = symbols(\"x\")\n",
    "\n",
    "display(\"Iterations as a function of x\")\n",
    "for k = 1:5\n",
    "    display(simplify(Babylonian(x; N=k)))\n",
    "end\n",
    "\n",
    "display(\"Derivatives as a function of x\")\n",
    "for k = 1:5\n",
    "    display(simplify(diff(simplify(Babylonian(x; N=k)), x)))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does AD get the answer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that we can rewrite our algorithm by taking the derivative of every operation to obtain the correct result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function dBabylonian(x; N=10)\n",
    "    t = (1+x)/2\n",
    "    dt = 1/2\n",
    "    for i = 2:N\n",
    "        t = (t + x/t)/2\n",
    "        dt = (dt + (t-x*dt)/t^2)/2;\n",
    "    end\n",
    "    dt\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dBabylonian(5), 0.5/√5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, basically the trick is for the computer system to do the rewrite for you, without any loss of speed or convenience."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Important: The derivative is substituted *before* the JIT compilation, and thus efficient compiled code is executed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ForwardDiff.jl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility function for our small forward AD\n",
    "derivative(f::Function, x::Number) = f(D(x, one(x))).ϵ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative(Babylonian, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "derivative(x -> 1/x, 4), -1/4^2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have understood how forward AD works, we can use the more feature complete package [ForwardDiff.jl](https://github.com/JuliaDiff/ForwardDiff.jl)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ForwardDiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ForwardDiff.derivative(Babylonian, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@edit ForwardDiff.derivative(Babylonian, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Note: [DiffRules.jl](https://github.com/JuliaDiff/DiffRules.jl))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**: pressure = - d/dV(free energy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some nice reads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Blog posts:\n",
    "\n",
    "* ML in Julia: https://julialang.org/blog/2018/12/ml-language-compiler\n",
    "\n",
    "* Nice example: https://fluxml.ai/2019/03/05/dp-vs-rl.html\n",
    "\n",
    "* Nice interactive examples: https://fluxml.ai/experiments/\n",
    "\n",
    "* Why Julia for ML? https://julialang.org/blog/2017/12/ml&pl\n",
    "\n",
    "* Neural networks with differential equation layers: https://julialang.org/blog/2019/01/fluxdiffeq\n",
    "\n",
    "* Implement Your Own Automatic Differentiation with Julia in ONE day : http://blog.rogerluo.me/2018/10/23/write-an-ad-in-one-day/\n",
    "\n",
    "* Implement Your Own Source To Source AD in ONE day!: http://blog.rogerluo.me/2019/07/27/yassad/\n",
    "\n",
    "Repositories:\n",
    "\n",
    "* AD flavors, like forward and reverse mode AD: https://github.com/MikeInnes/diff-zoo (Mike is one of the smartest Julia ML heads)\n",
    "\n",
    "Talks:\n",
    "\n",
    "* AD is a compiler problem: https://juliacomputing.com/assets/pdf/CGO_C4ML_talk.pdf"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
