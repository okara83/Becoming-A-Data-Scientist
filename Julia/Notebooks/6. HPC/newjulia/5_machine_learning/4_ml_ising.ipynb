{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning the Ising Transition\n",
    "### Carsten Bauer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [Ising model](https://en.wikipedia.org/wiki/Ising_model) is arguably the most famous model in (condensed matter) physics. It is described by the simple Hamiltonian\n",
    "\n",
    "$$H = - J \\sum_{\\langle i,j \\rangle} s_i s_j.$$\n",
    "\n",
    "Here, the $s_i=\\{-1,1\\}$ are classical, binary magnetic moments (spins) sitting on a two-dimensional square lattice and the $\\langle i,j \\rangle$ indicates that only interactions betweens neighboring spins are taken into account. For simplicity, we will set $J=1$.\n",
    "\n",
    "Most importantly, the Ising model shows a phase transition between a paramagnetic and a ferromagnetic phase as a function of temperature. The critical temperature $T_c$ at which this change of magnetic character occurs has been calculated exactly by [Lars Onsager](https://en.wikipedia.org/wiki/Lars_Onsager). He found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "const IsingTc = 1/(1/2*log(1+sqrt(2))) # Exact Onsager solution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we aim to reproduce this result (roughly) using a simple Neural Network. Basically, what we'll do was worth a [nature physics publication](https://www.nature.com/articles/nphys4035) not so long ago!\n",
    "\n",
    "We will start by quickly simulating the Ising model using the Monte Carlo method to obtain representative sets of spin configurations for a bunch of temperatures. Afterwards, we **do not** take the traditional approach of inspecting the magnetization, the order parameter of the transition, and its susceptibility. Instead we use supervised learning to train a simple Neural Network to automagically learn the transition temperature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Monte Carlo method for the Ising model is very straightforward: take a random configuration of spins to begin with and propose individual spin flips until you fall asleep. To decide whether a spin should be flipped we use the Metropolis criterium\n",
    "\n",
    "$$p = \\min{\\{1, e^{-\\beta\\Delta E}\\}}$$\n",
    "\n",
    "where $\\Delta E = E' - E$ is the energy difference between the new (spin flipped) and the old configuration according to $H$ above and $\\beta = 1/T$ is the inverse of the temperature $T$. Since $\\Delta E$ only depends on the local environment of the spin to be flipped (nearest neighbors), we can evaluate it locally. Without going into details, here is a simple implementation of the Monte Carlo idea for the Ising model. It is not optimize by any means, but it will serve its purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Printf, Dates\n",
    "\n",
    "# functions to obtain neighbors of a given site i\n",
    "up(neighs, i) = neighs[1, i]\n",
    "right(neighs, i) = neighs[2, i]\n",
    "down(neighs, i) = neighs[3, i]\n",
    "left(neighs, i) = neighs[4, i]\n",
    "\n",
    "function montecarlo(; L, T)\n",
    "    # set parameters & initialize\n",
    "    nsweeps = 10^7\n",
    "    measure_rate = 5_000\n",
    "    beta = 1/T\n",
    "    conf = rand([-1, 1], L, L)\n",
    "    confs = Matrix{Int64}[] # storing intermediate configurations\n",
    "    # build nearest neighbor lookup table\n",
    "    lattice = reshape(1:L^2, (L, L))\n",
    "    ups     = circshift(lattice, (-1,0))\n",
    "    rights  = circshift(lattice, (0,-1))\n",
    "    downs   = circshift(lattice,(1,0))\n",
    "    lefts   = circshift(lattice,(0,1))\n",
    "    neighs = vcat(ups[:]',rights[:]',downs[:]',lefts[:]')\n",
    "    \n",
    "    start_time = now()\n",
    "    println(\"Started: \", Dates.format(start_time, \"d.u yyyy HH:MM\"))\n",
    "    \n",
    "    # walk over the lattice and propose to flip each spin `nsweeps` times\n",
    "    for i in 1:nsweeps\n",
    "        # sweep\n",
    "        for i in eachindex(conf)\n",
    "            # calculate ΔE\n",
    "            ΔE = 2.0 * conf[i] * (conf[up(neighs, i)] + conf[right(neighs, i)] +\n",
    "                                + conf[down(neighs, i)] + conf[left(neighs, i)])\n",
    "            # Metropolis criterium\n",
    "            if ΔE <= 0 || rand() < exp(- beta*ΔE)\n",
    "                conf[i] *= -1 # flip spin\n",
    "            end\n",
    "        end\n",
    "        \n",
    "        # store the spin configuration\n",
    "        iszero(mod(i, measure_rate)) && push!(confs, copy(conf))\n",
    "    end\n",
    "    \n",
    "    end_time = now()\n",
    "    println(\"Ended: \", Dates.format(end_time, \"d.u yyyy HH:MM\"))\n",
    "    @printf(\"Duration: %.2f minutes\", (end_time - start_time).value / 1000. /60.)\n",
    "    \n",
    "    # return the recorded spin configurations\n",
    "    return confs\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we run this code for a small system with $L=8$ ($L$ is the linear dimension of a square lattice of size $L \\times L$), we obtain 2000 (approximately) representative spin configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "montecarlo(L=8, T=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate an $L=8$ system at a couple of temperatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, to identify the ferromagnetic phase transition as a function of temperature, we'll need at least a few more temperatures. For example, let's look at these temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ts = [1.189, 1.733, 2.069, 2.269, 2.278, 2.469, 2.822, 3.367]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This set contains the exact Onsager solution `IsingTc` and a bunch of temperature around it as shown here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize temperatures\n",
    "using Plots\n",
    "vline(Ts, grid=false, axis=:x, framestyle=:origin, xlim=(minimum(Ts)-0.1, maximum(Ts)+0.1), size=(800,200), label=\"Ts\")\n",
    "scatter!(Ts, fill(0, length(Ts)), color=:lightblue, label=\"\")\n",
    "vline!([IsingTc], color=:red, label=\"Tc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright, now that we are prepared let's run those simulations (takes about 4 minutes on my i5 desktop machine) and store the configurations in a `T=>confs` dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confs = Dict{Float64, Array{Float64,3}}() # key: T, value: confs\n",
    "for T in Ts\n",
    "    println(\"T = $T\"); flush(stdout);\n",
    "    c = montecarlo(L=8, T=T)\n",
    "    confs[T] = cat(c..., dims=3)\n",
    "    println(\"Done.\\n\")\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine learning the magnetic phase transition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How can we utilize machine learning to find $T_c$? Basically, we'll build upon a simple fact: even if we don't know the precise value of $T_c$, clearly, at high temperatures the system should be disorderd, i.e. paramagnetic, since temperature is wiggling our spins such that they cannot align. Similarly, for small $T$, where temperature fluctuations are frozen out, we should be in an ordered (ferromagnetic) phase, where the vast majority of spins points in the same direction.\n",
    "\n",
    "Ok, but how does that help? In the spirit of supervised learning, this knowledge allows us to put phase labels on some of our configurations, namely those for the highest (`Tright = 3.367`) and lowest (`Tleft = 1.189`) values of $T$. We'll consider the following simple neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"network.png\" width=400px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **green neurons** will be our input configurations. Note that we'll linearize our two-dimensional configurations, that is we'll just throw away the dimensionality information and take them each as a big, one-dimensional vector.\n",
    "\n",
    "The **red neurons** are the hidden neurons. We'll only have a single hidden layer of 10 neurons (only showing 5 above) which is densely connected (n-to-n) to our input neurons.\n",
    "\n",
    "The **blue neurons** are the output neurons which will indicate the network's confidence about whether we are in the paramagnetic or ferromagnetic phase (we'll use a [softmax activation](https://en.wikipedia.org/wiki/Softmax_function) to assure that their values sum up to 1).\n",
    "\n",
    "Our training strategy now is as follows. First, we feed the network the `Tleft` configurations and, based on our knowledge that we should be ordered at this temperature, optimize the network parameters with respect to producing a 1-0 output in favor of the ferromagnetic phase. Then, we do the same for the `Tright` configurations, where we expect to have a 0-1 prediction.\n",
    "\n",
    "Afterwards, we fix the weights and biases of the network and ask it for every intermediate temperature: How confident are you that we are in the paramagnetic/ferromagnetic phase? The point of maximal confusion, i.e. 0.5-0.5, will be what we'll identify as $T_c$.\n",
    "\n",
    "Let's get to it. We start by loading [Flux.jl](https://fluxml.ai/), the pure Julia machine-learning stack, and a few standard libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Flux\n",
    "using Flux: crossentropy, onecold, onehotbatch, params, throttle, @epochs\n",
    "using Statistics, Random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned above, we won't/can't feed the network two-dimensional input data but have to flatten the configurations. Also, to improve the data from our quick-and-dirty simulation above, we exploit the $Z_2$ symmetry of the Ising Hamiltonian $H$ which states that we can flip all spins collectively without changing the energy of the system ($s_i \\rightarrow -s_i$). This way, we effectively double our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function flatten_and_Z2(confs, T)\n",
    "    c = confs[T]\n",
    "    cs = Float64.(reshape(c, (64,:))) # flatten space dimension\n",
    "    cs = hcat(cs, -one(eltype(cs)) .* cs) # concatenate Z2 (spin flip) symmetry partners\n",
    "    return cs\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "L = 8\n",
    "Tleft = 1.189\n",
    "Tright = 3.367\n",
    "\n",
    "confs_left = flatten_and_Z2(confs, Tleft)\n",
    "confs_right = flatten_and_Z2(confs, Tright);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since visualizations are always a good thing, let's visualize the configurations at the lowest and highest temperatures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize configurations\n",
    "printconfs(confs) = plot([heatmap(Gray.(reshape(confs[:,i], (L,L))), ticks=false) for i in 1:100:size(confs, 2)]...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printconfs(confs_left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "printconfs(confs_right)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we can start defining our model and training it we have to group our data into the `(X, Y)` structure that Flux expects. Once again, to fake improve our data, we repeat the data we have 10 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up as training data\n",
    "neach = size(confs_left, 2)\n",
    "X = hcat(confs_left, confs_right)\n",
    "labels = vcat(fill(1, neach), fill(0, neach))\n",
    "Y = onehotbatch(labels, 0:1)\n",
    "dataset = Base.Iterators.repeated((X, Y), 10); # repeat dataset 10 times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alright. So far, we have only prepared our data. Let's finally define our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Random.seed!(123)\n",
    "\n",
    "m = Chain(\n",
    "    Dense(L^2, 10, relu), # 10 hidden neurons\n",
    "    Dense(10, 2), # two output neurons for paramagnetic vs ferromagnetic\n",
    "    softmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, the network parameters are random and the network has no predictive power. A good way to visualize this fact is a *confidence plot*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classify phases at all intermediate temperatures\n",
    "function confidence_plot()\n",
    "    results = Dict{Float64, Vector{Float32}}()\n",
    "    for T in Ts\n",
    "      c = flatten_and_Z2(confs, T);\n",
    "      results[T] = vec(mean(m(c), dims=2).data)\n",
    "    end\n",
    "    results = sort(results)\n",
    "\n",
    "    p = plot(keys(results) |> collect, reduce(hcat, values(results))',\n",
    "      marker=:circle,\n",
    "      xlab=\"temperature\",\n",
    "      ylabel=\"CNN confidence\",\n",
    "      labels=[\"paramagnet\", \"ferromagnet\"],\n",
    "      frame=:box)\n",
    "    plot!(p, [IsingTc, IsingTc], [0, 1], ls=:dash, color=:black, label=\"IsingTc\")\n",
    "    if (@isdefined IJulia)\n",
    "        # \"animation\" in jupyter\n",
    "        IJulia.clear_output(true)\n",
    "    end\n",
    "    display(p)\n",
    "end\n",
    "\n",
    "confidence_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network clearly has no idea in which phase we are. See what happens after training it for a couple of seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define cost-function\n",
    "loss(x, y) = crossentropy(m(x), y)\n",
    "\n",
    "# define optimizer\n",
    "opt = ADAM()\n",
    "\n",
    "# train for 100 epochs\n",
    "for i in 1:100\n",
    "    Flux.train!(loss, params(m), dataset, opt)\n",
    "end\n",
    "\n",
    "# \n",
    "confidence_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boom. The point of maximal confusion agrees with the exact Onsager solution remarkably well.\n",
    "\n",
    "A couple of things have to be noted.\n",
    "\n",
    "* The quality of the result might depend a bit on where (at which temperatures) we train the network.\n",
    "* We are somewhat stretching things here as our system is tiny ($L=8$) and finite-size effects are expected. After all, the phase transition is a property of the infinite system.\n",
    "* Also, our simulation was quick-and-dirty and our configurations might not be actually representitive. We'd have to use cluster updates and run simulations longer to be safe.\n",
    "\n",
    "Despite those points, we have seen that **Monte Carlo + Machine Learning can be used to identify phase transitions** in a physical system - a new field that is interesting and exciting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Optional:** In Jupyter notebooks and Juno, we can visualize the learning process by updating the confidence plot during learning via a *callback* (uncomment and run the following code)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a callback\n",
    "# evalcb = () -> begin\n",
    "# #     @show(loss(X, Y))\n",
    "# #     @show(accuracy(X, Y))\n",
    "#     confidence_plot()\n",
    "# end\n",
    "\n",
    "# # Reset the network and the optimizer\n",
    "# Random.seed!(123)\n",
    "# m = Chain(\n",
    "#     Dense(L^2, 10, relu),\n",
    "#     Dense(10, 2),\n",
    "#     softmax)\n",
    "# opt = ADAM()\n",
    "\n",
    "# # Train for 100 epochs (with \"animation\")\n",
    "# for i in 1:100\n",
    "#     Flux.train!(loss, params(m), dataset, opt, cb = throttle(evalcb, 50))\n",
    "# end"
   ]
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "Julia 1.2.0",
   "language": "julia",
   "name": "julia-1.2"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.2.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
